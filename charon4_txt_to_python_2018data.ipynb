{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Charon4 txt to Python\n",
    "\n",
    "The results of this script are stored in `..\\Alsdorf\\Daten\\my_database`. The goal of this database is to contain all data in an easy to read format.\n",
    "\n",
    "# Import 2018 data; different export settings\n",
    "The following screenshot shows the expected settings while exporting the 2018 data. They are exported per day, for easier reading of the different cable lengths.\n",
    "\n",
    "<img src=\".\\pictures\\Charon4_export_window_txt_2018_day.png\" alt=\"drawing\" width=\"500\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do \n",
    "# TRT Daten von Sciebo hinzufÃ¼gen\n",
    "# Temp. Sonde in Eduardschacht angucken und vergleichen\n",
    "\n",
    "version=\"v1_1\"\n",
    "# Changelog\n",
    "#v1_0: extracted this notebook from charon4_txt_to_python\n",
    "# resolved a problem where the data with 0.5 sampling was imported into a to big dataframe; \n",
    "# pickle and csv data are now saved with the same format\n",
    "#v1_1: removed measurement per day plot from this script and added it to my_database_script\n",
    "\n",
    "# not all of these are needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib import colors\n",
    "from datetime import date, timedelta\n",
    "from collections import defaultdict\n",
    "#from collections import Counter\n",
    "#import matplotlib.patches as patches\n",
    "#import matplotlib.dates as mdates\n",
    "#from shutil import copy2 as copy_file\n",
    "import pickle\n",
    "\n",
    "from my_func_mvw.functions import get_abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "#path to working directory\n",
    "#mit \\[-]\\ im Pfad funktioniert die get_abspath Funktion nicht\n",
    "\n",
    "##############Input##############################\n",
    "path_to_controller=r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\" #also used for saving plots\n",
    "data_save_csv    =True # True False; data saving takes about 1 min.\n",
    "data_save_pickle =True # True False; takes less than 1 s.\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller=3188 # 3188: Alsdorf\n",
    "# generate all requested paths, some paths may be empty\n",
    "path_to_controller_2018_day=r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\2018_export_by_day\"\n",
    "year=\"2018\"\n",
    "channels=[1,2,3,4,5,6,7,8] #[1,2,3,4,5,6,7,8]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12] #[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "paths_for_activate_2018_day={}\n",
    "\n",
    "for channel in channels:\n",
    "    for month in months:\n",
    "        paths_for_activate_2018_day[f\"temp_ch{channel}_year2018_month{month}\"] = path_to_controller_2018_day + f\"\\Controller\\{controller}\\{channel}\\Temperature Data\\{year}\\{month}\"\n",
    "\n",
    "#paths_for_activate_2018_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_temp_to_df_2018(path):\n",
    "    \"\"\"\"\"\"\n",
    "    one_file = pd.read_csv(path,decimal=\".\",delimiter=\"\\t\",skiprows=7,index_col=0)\n",
    "    one_file = one_file.drop(one_file.columns[0:2],axis=1)\n",
    "    one_file.index = pd.to_datetime(one_file.index, infer_datetime_format=True)\n",
    "    #one_file.rename(columns=lambda x: float(x), inplace=True) # when sampling is 0.5 m, int values are not sufficient\n",
    "    # thats easier to read, and has the same result?\n",
    "    one_file.columns = one_file.columns.astype(float)\n",
    "    one_file.index.names = ['Date']\n",
    "    one_file.columns.names=[\"Length [m]\"]\n",
    "    return one_file\n",
    "\n",
    "def create_base_dataframe(n_columns,sampling_05=False):\n",
    "    \"\"\"\"\"\"\n",
    "    if sampling_05==False: #sampling interval is 1 m\n",
    "        df=pd.DataFrame(columns=np.linspace(0,n_columns-1,n_columns))\n",
    "        df.rename(columns=lambda x: float(x), inplace=True) #to have all 2018 data data as float\n",
    "    elif sampling_05==True:\n",
    "        df=pd.DataFrame(columns=np.linspace(0,(n_columns-1)/2,n_columns))\n",
    "    df.index.names = ['Date']\n",
    "    df.columns.names=[\"Length [m]\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 ParserErrors were skipped. This means 2 days with data are not read!\nWall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Wall time: 2min 17s\n",
    "\n",
    "# TO-DO !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# check if one day folder contains more than one file\n",
    "\n",
    "# create dic depending on channelnumber and cable length\n",
    "cable_lengths_2018=[233,1314,2340,1171,1170,798,259,799,268,1352,676,223,378,250,800]\n",
    "#all_files=defaultdict(dict) #two level defaultdict\n",
    "#all_files = defaultdict(lambda: defaultdict(dict)) ##two level defaultdict, different style\n",
    "# data_2018_day, is named day because the data is exported different from Charon4, than the data from the other years\n",
    "data_2018_day=my_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) #three level defaultdict\n",
    "for channel in channels:\n",
    "    for cable_type in cable_lengths_2018:\n",
    "        for month in months:\n",
    "\n",
    "            if cable_type in [233,1314,1171,1170,798,259,799,268,676,223,378,250,800]: # sampling interval is 1 m\n",
    "                data_2018_day[cable_type][str(channel)][str(month)] = create_base_dataframe(cable_type)\n",
    "\n",
    "            elif cable_type in [2340, 1352]: #sampling interval is 0.5 m instead of 1 m\n",
    "                data_2018_day[cable_type][str(channel)][str(month)] = create_base_dataframe(cable_type,sampling_05=True)\n",
    "\n",
    "# Read the data\n",
    "count_ParserError=0\n",
    "for channel in channels:\n",
    "    for month in months: #path could be empty --> no data \n",
    "        my_path=paths_for_activate_2018_day[f\"temp_ch{channel}_year2018_month{month}\"]\n",
    "        all_my_paths=get_abspath(my_path + \"\\*\\*\") \n",
    "\n",
    "        for file in all_my_paths:\n",
    "            # check cable length and create different dataframes\n",
    "\n",
    "            try: \n",
    "                # Read Data\n",
    "                one_file = import_temp_to_df_2018(file)\n",
    "\n",
    "                #check cable length and put it in corresponding dataframe\n",
    "                n_columns=len(one_file.columns)\n",
    "                data_2018_day[n_columns][str(channel)][str(month)]=pd.concat([data_2018_day[n_columns][str(channel)][str(month)],one_file],axis=0)\n",
    "                # doppelte datums tauchen auf\n",
    "\n",
    "            # except ParserError, file contains data with different cable lengths\n",
    "            # With this solution I will skip these data.\n",
    "            except pd.errors.ParserError:\n",
    "                count_ParserError+=1\n",
    "                pass\n",
    "\n",
    "print(f\"{count_ParserError} ParserErrors were skipped. This means {count_ParserError} days with data are not read!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_helper(data_2018_day,channel,cable_length,create_base_dataframe=create_base_dataframe):\n",
    "    \"\"\"merges different month for each channel into one dataframe\"\"\"\n",
    "    if cable_length in [233,1314,1171,1170,798,259,799,268,676,223,378,250,800]: # sampling interval is 1 m\n",
    "        data_2018_day_save = create_base_dataframe(cable_length)\n",
    "    elif cable_length in [2340, 1352]: #sampling interval is 0.5 m instead of 1 m\n",
    "        data_2018_day_save = create_base_dataframe(cable_length,sampling_05=True)\n",
    "\n",
    "    for month in data_2018_day[cable_length][channel].keys():\n",
    "        one_file = data_2018_day[cable_length][channel][month]\n",
    "        data_2018_day_save = pd.concat([data_2018_day_save , one_file],axis=0) \n",
    "\n",
    "    return data_2018_day_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Wall time: 49.1 s\n",
    "# Save Data 2018 csv\n",
    "if data_save_csv:\n",
    "    save_to_path = path_to_controller + r\"\\..\\..\\my_database\\temp_2018_Controller3188\\csv\"\n",
    "    #path_to_controller + \"\\..\\..\\my_database\"\n",
    "    for cable_length in data_2018_day.keys():\n",
    "        for channel in data_2018_day[cable_length].keys():\n",
    "            data_2018_day_save = save_helper(data_2018_day,channel,cable_length)\n",
    "\n",
    "            # Save data\n",
    "            if data_2018_day_save.shape[0]!=0: # dataframe contains rows (with data)\n",
    "                filename=f\"\\\\temp_cablelength{cable_length}_ch{channel}.csv\"\n",
    "                data_2018_day_save.to_csv(save_to_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Wall time: 11.7 s\n",
    "# Save Data 2018 pickle\n",
    "if data_save_pickle:\n",
    "    def write_pickle(save_to:str,data_2018_day):\n",
    "        #Function to write pickle Files\n",
    "        for cable_length in data_2018_day.keys():\n",
    "            for channel in data_2018_day[cable_length].keys():\n",
    "                data_2018_day_save = save_helper(data_2018_day,channel,cable_length)\n",
    "\n",
    "                #if len(data_2018_day[cable_length][channel].keys()) != 0:\n",
    "                if data_2018_day_save.shape[0]!=0: # dataframe contains rows (with data)\n",
    "                    filename=f\"\\\\temp_cablelength{cable_length}_ch{channel}\"\n",
    "                    with open(save_to + filename, 'wb') as handle:\n",
    "                        pickle.dump(data_2018_day_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    save_to_path_pickle = path_to_controller + r\"\\..\\..\\my_database\\temp_2018_Controller3188\\pickle\"\n",
    "    write_pickle(save_to_path_pickle,data_2018_day)\n",
    "\n",
    "\n",
    "    # Load pickle data - test\n",
    "    def read_pickle(filename:str):\n",
    "        #Function to read pickle Files\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    filename=r\"\\temp_cablelength378_ch5\"\n",
    "    load_file_pickle = save_to_path_pickle + filename\n",
    "    test_load_pickle=read_pickle(load_file_pickle)\n",
    "    #test_load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}