{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Charon4 txt to Python, 2018 data\r\n",
    "\r\n",
    "The results of this script are stored in `..\\Alsdorf\\Daten\\my_database`. The goal of this database is to contain all data in an easy to read format.\r\n",
    "\r\n",
    "# different export settings compared to the recent data!\r\n",
    "The following screenshot shows the expected settings while exporting the 2018 data. They are exported per day, for easier reading of the different cable lengths.\r\n",
    "\r\n",
    "<img src=\".\\pictures\\Charon4_export_window_txt_2018_day.png\" alt=\"drawing\" width=\"500\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# To-Do \r\n",
    "# TRT Daten von Sciebo hinzufÃ¼gen\r\n",
    "# Temp. Sonde in Eduardschacht angucken und vergleichen\r\n",
    "\r\n",
    "version=\"v1_1\"\r\n",
    "# Changelog\r\n",
    "#v1_0: extracted this notebook from charon4_txt_to_python\r\n",
    "# resolved a problem where the data with 0.5 sampling was imported into a to big dataframe; \r\n",
    "# pickle and csv data are now saved with the same format\r\n",
    "#v1_1: removed measurement per day plot from this script and added it to my_database_script\r\n",
    "\r\n",
    "# not all of these are needed\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "#from matplotlib import colors\r\n",
    "from datetime import date, timedelta\r\n",
    "from collections import defaultdict\r\n",
    "#from collections import Counter\r\n",
    "#import matplotlib.patches as patches\r\n",
    "#import matplotlib.dates as mdates\r\n",
    "#from shutil import copy2 as copy_file\r\n",
    "import pickle\r\n",
    "\r\n",
    "from my_func_mvw.functions import get_abspath"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Input\r\n",
    "#path to working directory\r\n",
    "#mit \\[-]\\ im Pfad funktioniert die get_abspath Funktion nicht\r\n",
    "\r\n",
    "##############Input##############################\r\n",
    "path_to_controller=r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\" #also used for saving plots\r\n",
    "data_save_csv    =True # True False; data saving takes about 1 min.\r\n",
    "data_save_pickle =True # True False; takes less than 1 s.\r\n",
    "##################################################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "controller=3188 # 3188: Alsdorf\r\n",
    "# generate all requested paths, some paths may be empty\r\n",
    "path_to_controller_2018_day=r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\2018_export_by_day\"\r\n",
    "year=\"2018\"\r\n",
    "channels=[1,2,3,4,5,6,7,8] #[1,2,3,4,5,6,7,8]\r\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12] #[1,2,3,4,5,6,7,8,9,10,11,12]\r\n",
    "paths_for_activate_2018_day={}\r\n",
    "\r\n",
    "for channel in channels:\r\n",
    "    for month in months:\r\n",
    "        paths_for_activate_2018_day[f\"temp_ch{channel}_year2018_month{month}\"] = path_to_controller_2018_day + f\"\\Controller\\{controller}\\{channel}\\Temperature Data\\{year}\\{month}\"\r\n",
    "\r\n",
    "#paths_for_activate_2018_day"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def import_temp_to_df_2018(path):\r\n",
    "    \"\"\"\"\"\"\r\n",
    "    one_file = pd.read_csv(path,decimal=\".\",delimiter=\"\\t\",skiprows=7,index_col=0)\r\n",
    "    one_file = one_file.drop(one_file.columns[0:2],axis=1)\r\n",
    "    one_file.index = pd.to_datetime(one_file.index, infer_datetime_format=True)\r\n",
    "    #one_file.rename(columns=lambda x: float(x), inplace=True) # when sampling is 0.5 m, int values are not sufficient\r\n",
    "    # thats easier to read, and has the same result?\r\n",
    "    one_file.columns = one_file.columns.astype(float)\r\n",
    "    one_file.index.names = ['Date']\r\n",
    "    one_file.columns.names=[\"Length [m]\"]\r\n",
    "    return one_file\r\n",
    "\r\n",
    "def create_base_dataframe(n_columns,sampling_05=False):\r\n",
    "    \"\"\"\"\"\"\r\n",
    "    if sampling_05==False: #sampling interval is 1 m\r\n",
    "        df=pd.DataFrame(columns=np.linspace(0,n_columns-1,n_columns))\r\n",
    "        df.rename(columns=lambda x: float(x), inplace=True) #to have all 2018 data data as float\r\n",
    "    elif sampling_05==True:\r\n",
    "        df=pd.DataFrame(columns=np.linspace(0,(n_columns-1)/2,n_columns))\r\n",
    "    df.index.names = ['Date']\r\n",
    "    df.columns.names=[\"Length [m]\"]\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%%time\n",
    "#Wall time: 2min 17s\n",
    "\n",
    "# TO-DO !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# check if one day folder contains more than one file\n",
    "\n",
    "# create dic depending on channelnumber and cable length\n",
    "cable_lengths_2018=[233,1314,2340,1171,1170,798,259,799,268,1352,676,223,378,250,800]\n",
    "#all_files=defaultdict(dict) #two level defaultdict\n",
    "#all_files = defaultdict(lambda: defaultdict(dict)) ##two level defaultdict, different style\n",
    "# data_2018_day, is named day because the data is exported different from Charon4, than the data from the other years\n",
    "data_2018_day=my_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) #three level defaultdict\n",
    "for channel in channels:\n",
    "    for cable_type in cable_lengths_2018:\n",
    "        for month in months:\n",
    "\n",
    "            if cable_type in [233,1314,1171,1170,798,259,799,268,676,223,378,250,800]: # sampling interval is 1 m\n",
    "                data_2018_day[cable_type][str(channel)][str(month)] = create_base_dataframe(cable_type)\n",
    "\n",
    "            elif cable_type in [2340, 1352]: #sampling interval is 0.5 m instead of 1 m\n",
    "                data_2018_day[cable_type][str(channel)][str(month)] = create_base_dataframe(cable_type,sampling_05=True)\n",
    "\n",
    "# Read the data\n",
    "count_ParserError=0\n",
    "for channel in channels:\n",
    "    for month in months: #path could be empty --> no data \n",
    "        my_path=paths_for_activate_2018_day[f\"temp_ch{channel}_year2018_month{month}\"]\n",
    "        all_my_paths=get_abspath(my_path + \"\\*\\*\") \n",
    "\n",
    "        for file in all_my_paths:\n",
    "            # check cable length and create different dataframes\n",
    "\n",
    "            try: \n",
    "                # Read Data\n",
    "                one_file = import_temp_to_df_2018(file)\n",
    "\n",
    "                #check cable length and put it in corresponding dataframe\n",
    "                n_columns=len(one_file.columns)\n",
    "                data_2018_day[n_columns][str(channel)][str(month)]=pd.concat([data_2018_day[n_columns][str(channel)][str(month)],one_file],axis=0)\n",
    "                # doppelte datums tauchen auf\n",
    "\n",
    "            # except ParserError, file contains data with different cable lengths\n",
    "            # With this solution I will skip these data.\n",
    "            except pd.errors.ParserError:\n",
    "                count_ParserError+=1\n",
    "                pass\n",
    "\n",
    "print(f\"{count_ParserError} ParserErrors were skipped. This means {count_ParserError} days with data are not read!\")        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 ParserErrors were skipped. This means 2 days with data are not read!\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def save_helper(data_2018_day,channel,cable_length,create_base_dataframe=create_base_dataframe):\n",
    "    \"\"\"merges different month for each channel into one dataframe\"\"\"\n",
    "    if cable_length in [233,1314,1171,1170,798,259,799,268,676,223,378,250,800]: # sampling interval is 1 m\n",
    "        data_2018_day_save = create_base_dataframe(cable_length)\n",
    "    elif cable_length in [2340, 1352]: #sampling interval is 0.5 m instead of 1 m\n",
    "        data_2018_day_save = create_base_dataframe(cable_length,sampling_05=True)\n",
    "\n",
    "    for month in data_2018_day[cable_length][channel].keys():\n",
    "        one_file = data_2018_day[cable_length][channel][month]\n",
    "        data_2018_day_save = pd.concat([data_2018_day_save , one_file],axis=0) \n",
    "\n",
    "    return data_2018_day_save"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%%time\n",
    "#Wall time: 49.1 s\n",
    "# Save Data 2018 csv\n",
    "if data_save_csv:\n",
    "    save_to_path = path_to_controller + r\"\\..\\..\\my_database\\temp_2018_Controller3188\\csv\"\n",
    "    #path_to_controller + \"\\..\\..\\my_database\"\n",
    "    for cable_length in data_2018_day.keys():\n",
    "        for channel in data_2018_day[cable_length].keys():\n",
    "            data_2018_day_save = save_helper(data_2018_day,channel,cable_length)\n",
    "\n",
    "            # Save data\n",
    "            if data_2018_day_save.shape[0]!=0: # dataframe contains rows (with data)\n",
    "                filename=f\"\\\\temp_cablelength{cable_length}_ch{channel}.csv\"\n",
    "                data_2018_day_save.to_csv(save_to_path + filename)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%time\n",
    "#Wall time: 11.7 s\n",
    "# Save Data 2018 pickle\n",
    "if data_save_pickle:\n",
    "    def write_pickle(save_to:str,data_2018_day):\n",
    "        #Function to write pickle Files\n",
    "        for cable_length in data_2018_day.keys():\n",
    "            for channel in data_2018_day[cable_length].keys():\n",
    "                data_2018_day_save = save_helper(data_2018_day,channel,cable_length)\n",
    "\n",
    "                #if len(data_2018_day[cable_length][channel].keys()) != 0:\n",
    "                if data_2018_day_save.shape[0]!=0: # dataframe contains rows (with data)\n",
    "                    filename=f\"\\\\temp_cablelength{cable_length}_ch{channel}\"\n",
    "                    with open(save_to + filename, 'wb') as handle:\n",
    "                        pickle.dump(data_2018_day_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    save_to_path_pickle = path_to_controller + r\"\\..\\..\\my_database\\temp_2018_Controller3188\\pickle\"\n",
    "    write_pickle(save_to_path_pickle,data_2018_day)\n",
    "\n",
    "\n",
    "    # Load pickle data - test\n",
    "    def read_pickle(filename:str):\n",
    "        #Function to read pickle Files\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    filename=r\"\\temp_cablelength378_ch5\"\n",
    "    load_file_pickle = save_to_path_pickle + filename\n",
    "    test_load_pickle=read_pickle(load_file_pickle)\n",
    "    #test_load_pickle"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 16 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}