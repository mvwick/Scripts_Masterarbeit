{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('Mathis': virtualenv)"
  },
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Charon4 txt to Python, 2018 data\r\n",
    "\r\n",
    "# different export settings compared to the recent data!\r\n",
    "The following screenshot shows the expected settings while exporting the 2018 data. They are exported per day, for easier reading of the different cable lengths.\r\n",
    "\r\n",
    "<img src=\"./pictures/Charon4_export_window_txt_2018_day.png\" alt=\"drawing\" width=\"1300\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from datetime import date, timedelta\r\n",
    "from collections import defaultdict\r\n",
    "import pickle\r\n",
    "\r\n",
    "from my_func_mvw.functions import get_abspath, read_pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "##############Input##############################\r\n",
    "path_DTS_processed = r\"..\\Alsdorf\\Daten\\DTS_processed\"\r\n",
    "data_save_csv    =False # True False\r\n",
    "data_save_pickle =False # True False\r\n",
    "##################################################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "controller=3188 # 3188: Alsdorf\r\n",
    "# generate all requested paths, some paths may be empty\r\n",
    "path_to_controller_2018_day=path_DTS_processed + r\"\\..\\DTS_unprocessed\\Charon4\\charon4_export_as_txt\\2018_export_by_day\"\r\n",
    "year=\"2018\"\r\n",
    "channels=[1,2,3,4,5,6,7,8]\r\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\r\n",
    "paths_for_activate_2018_day={}\r\n",
    "\r\n",
    "for channel in channels:\r\n",
    "    for month in months:\r\n",
    "        paths_for_activate_2018_day[f\"temp_ch{channel}_year2018_month{month}\"] = path_to_controller_2018_day + f\"\\Controller\\{controller}\\{channel}\\Temperature Data\\{year}\\{month}\"\r\n",
    "#paths_for_activate_2018_day"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def import_temp_to_df_2018(path):\r\n",
    "    \"\"\"\"\"\"\r\n",
    "    one_file = pd.read_csv(path,decimal=\".\",delimiter=\"\\t\",skiprows=7,index_col=0)\r\n",
    "    one_file = one_file.drop(one_file.columns[0:2],axis=1)\r\n",
    "    one_file.index = pd.to_datetime(one_file.index, infer_datetime_format=True).tz_localize(None)\r\n",
    "    one_file.columns = one_file.columns.astype(float)\r\n",
    "    one_file.index.names = ['Date']\r\n",
    "    one_file.columns.names=[\"Length [m]\"]\r\n",
    "    return one_file\r\n",
    "\r\n",
    "def create_base_dataframe(n_columns,sampling_05=False):\r\n",
    "    \"\"\"\"\"\"\r\n",
    "    if sampling_05==False: #sampling interval is 1 m\r\n",
    "        df=pd.DataFrame(columns=np.linspace(0,n_columns-1,n_columns))\r\n",
    "        df.rename(columns=lambda x: float(x), inplace=True) #to have all 2018 data data as float\r\n",
    "    elif sampling_05==True:\r\n",
    "        df=pd.DataFrame(columns=np.linspace(0,(n_columns-1)/2,n_columns))\r\n",
    "    df.index.names = ['Date']\r\n",
    "    df.columns.names=[\"Length [m]\"]\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Read all files exported by Charon\r\n",
    "\r\n",
    "# Create base dic: depending on channel number and cable length\r\n",
    "cable_lengths_2018=[233,1314,2340,1171,1170,798,259,799,268,1352,676,223,378,250,800]\r\n",
    "# data_2018_day, is named day because the data is exported different from Charon4, than the data from the other years\r\n",
    "data_2018_day=my_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) #three level defaultdict\r\n",
    "for channel in channels:\r\n",
    "    for cable_type in cable_lengths_2018:\r\n",
    "        for month in months:\r\n",
    "\r\n",
    "            if cable_type in [233,1314,1171,1170,798,259,799,268,676,223,378,250,800]: # sampling interval is 1 m\r\n",
    "                data_2018_day[cable_type][str(channel)][str(month)] = create_base_dataframe(cable_type)\r\n",
    "\r\n",
    "            elif cable_type in [2340, 1352]: #sampling interval is 0.5 m instead of 1 m\r\n",
    "                data_2018_day[cable_type][str(channel)][str(month)] = create_base_dataframe(cable_type,sampling_05=True)\r\n",
    "\r\n",
    "# Read the data\r\n",
    "count_ParserError=0\r\n",
    "for channel in channels:\r\n",
    "    for month in months: #path could be empty --> no data \r\n",
    "        my_path=paths_for_activate_2018_day[f\"temp_ch{channel}_year2018_month{month}\"]\r\n",
    "        all_my_paths=get_abspath(my_path + \"\\*\\*\") \r\n",
    "\r\n",
    "        for my_file in all_my_paths: # check cable length and create different dataframes\r\n",
    "            try: # Read Data\r\n",
    "                one_file = import_temp_to_df_2018(my_file)\r\n",
    "\r\n",
    "                #check cable length and put it in corresponding dataframe\r\n",
    "                n_columns=len(one_file.columns)\r\n",
    "                data_2018_day[n_columns][str(channel)][str(month)]=pd.concat([data_2018_day[n_columns][str(channel)][str(month)],one_file],axis=0)\r\n",
    "                # doppelte datums tauchen auf\r\n",
    "\r\n",
    "            except pd.errors.ParserError: # except ParserError, file contains data with different cable lengths\r\n",
    "                # With this solution I will skip these data.\r\n",
    "                count_ParserError+=1\r\n",
    "                pass\r\n",
    "        \r\n",
    "        # check if one folder contains more than one file\r\n",
    "        check_for_multiple_files=True\r\n",
    "        if check_for_multiple_files:\r\n",
    "            path_to_day_folders=get_abspath(my_path + \"\\*\")\r\n",
    "            for day_folder in path_to_day_folders:\r\n",
    "                n_files_in_dayfolder=len(get_abspath(day_folder + \"\\*\"))\r\n",
    "                if n_files_in_dayfolder != 1:\r\n",
    "                    print(\"Warning: a day folder conatins mpre than one file\")\r\n",
    "\r\n",
    "print(f\"{count_ParserError} ParserErrors were skipped. This means {count_ParserError} days with data are not read!\")        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 ParserErrors were skipped. This means 2 days with data are not read!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def save_helper(data_2018_day,channel,cable_length,create_base_dataframe=create_base_dataframe):\r\n",
    "    \"\"\"merges different month for each channel into one dataframe\"\"\"\r\n",
    "    if cable_length in [233,1314,1171,1170,798,259,799,268,676,223,378,250,800]: # sampling interval is 1 m\r\n",
    "        data_2018_day_save = create_base_dataframe(cable_length)\r\n",
    "    elif cable_length in [2340, 1352]: #sampling interval is 0.5 m instead of 1 m\r\n",
    "        data_2018_day_save = create_base_dataframe(cable_length,sampling_05=True)\r\n",
    "\r\n",
    "    for month in data_2018_day[cable_length][channel].keys():\r\n",
    "        one_file = data_2018_day[cable_length][channel][month]\r\n",
    "        data_2018_day_save = pd.concat([data_2018_day_save , one_file],axis=0) \r\n",
    "\r\n",
    "    return data_2018_day_save"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Save Data 2018 csv\r\n",
    "if data_save_csv:\r\n",
    "    save_to_path = path_DTS_processed + r\"\\temp_2018_Controller3188\\csv\"\r\n",
    "    for cable_length in data_2018_day.keys():\r\n",
    "        for channel in data_2018_day[cable_length].keys():\r\n",
    "            data_2018_day_save = save_helper(data_2018_day,channel,cable_length)\r\n",
    "\r\n",
    "            # Save data\r\n",
    "            if data_2018_day_save.shape[0]!=0: # dataframe contains rows (with data)\r\n",
    "                filename=f\"\\\\temp_cablelength{cable_length}_ch{channel}.csv\"\r\n",
    "                data_2018_day_save.to_csv(save_to_path + filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Save Data 2018 pickle\r\n",
    "if data_save_pickle:\r\n",
    "    def write_pickle(save_to:str,data_2018_day):\r\n",
    "        #Function to write pickle Files\r\n",
    "        for cable_length in data_2018_day.keys():\r\n",
    "            for channel in data_2018_day[cable_length].keys():\r\n",
    "                data_2018_day_save = save_helper(data_2018_day,channel,cable_length)\r\n",
    "\r\n",
    "                #if len(data_2018_day[cable_length][channel].keys()) != 0:\r\n",
    "                if data_2018_day_save.shape[0]!=0: # dataframe contains rows (with data)\r\n",
    "                    filename=f\"\\\\temp_cablelength{cable_length}_ch{channel}\"\r\n",
    "                    with open(save_to + filename, 'wb') as handle:\r\n",
    "                        pickle.dump(data_2018_day_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "\r\n",
    "    save_to_path_pickle = path_DTS_processed + r\"\\temp_2018_Controller3188\\pickle\"\r\n",
    "    write_pickle(save_to_path_pickle,data_2018_day)\r\n",
    "\r\n",
    "    # Load pickle data - test\r\n",
    "    filename=r\"\\temp_cablelength378_ch5\"\r\n",
    "    load_file_pickle = save_to_path_pickle + filename\r\n",
    "    test_load_pickle=read_pickle(load_file_pickle)\r\n",
    "    #test_load_pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Check\r\n",
    "if data_save_pickle: # use some variables, which I only define when saving pickle\r\n",
    "    # TRT data from sciebo: sciebo\\DTS Data\\Alsdorf\\Nullmessung_und_TRT_2018 - check if equal to the data here\r\n",
    "    # I only compare Ch 2 TRT currently. I assume the other ones will be also the same.\r\n",
    "    # Would need a bit more hacking to compare these too because the cable length changes so I would have to select multiple files of my database\r\n",
    "\r\n",
    "    trt_data_sciebo_ch2={}\r\n",
    "    trt_data_sciebo_ch1={}\r\n",
    "\r\n",
    "    #path = r\"..\\Alsdorf\\Nullmessung_und_TRT_2018\\Natural State\" # here multiple files would have to be compared because of cable length change\r\n",
    "    path = r\"..\\Alsdorf\\Nullmessung_und_TRT_2018\\TRT\"\r\n",
    "\r\n",
    "    path_to_files_ch2 = get_abspath(path + \"\\CH_2\\*\")\r\n",
    "    # path_to_files_ch1 = get_abspath(path + \"\\CH_1\\*\")\r\n",
    "    for filename in path_to_files_ch2:\r\n",
    "        trt_data_sciebo_ch2[filename] = pd.read_csv(filename,delimiter = ',',index_col=0, header=7 )\r\n",
    "        trt_data_sciebo_ch2[filename].index = pd.to_datetime(trt_data_sciebo_ch2[filename].index, dayfirst = True).tz_localize(None)\r\n",
    "        trt_data_sciebo_ch2[filename] = trt_data_sciebo_ch2[filename].drop(trt_data_sciebo_ch2[filename].columns[[0,1]], axis=1)\r\n",
    "    # for filename in path_to_files_ch1:\r\n",
    "    #     trt_data_sciebo_ch1[filename] = pd.read_csv(filename,delimiter = ',',index_col=0, header=7 )\r\n",
    "    #     trt_data_sciebo_ch1[filename].index = pd.to_datetime(trt_data_sciebo_ch1[filename].index, dayfirst = True).tz_localize(None)\r\n",
    "    #     trt_data_sciebo_ch1[filename] = trt_data_sciebo_ch1[filename].drop(trt_data_sciebo_ch1[filename].columns[[0,1]], axis=1)\r\n",
    "\r\n",
    "\r\n",
    "    data_in_database=read_pickle(save_to_path_pickle + r\"\\temp_cablelength676_ch2\")\r\n",
    "\r\n",
    "    for filepath in path_to_files_ch2:\r\n",
    "        once_sciebo_file=trt_data_sciebo_ch2[filepath]\r\n",
    "        once_sciebo_file.columns=data_in_database.columns # so comparison can be easy made\r\n",
    "\r\n",
    "        values_equal=True\r\n",
    "        for date in once_sciebo_file.index:\r\n",
    "            compare = once_sciebo_file.loc[date] == data_in_database.loc[date]\r\n",
    "            if compare.sum() == len(compare): #all values are equal\r\n",
    "                pass\r\n",
    "            else:\r\n",
    "                values_equal=False\r\n",
    "\r\n",
    "    print(f\"all values of channel 2 are equal: {values_equal}\\nsciebo flies from {path}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}