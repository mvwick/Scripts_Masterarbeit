{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tlogger data\n",
    "here I will import the raw tlogger data and save them in an easy to acces format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Version information of the imported packages\n",
      "pandas version: 1.2.3\n",
      "pickle version: 4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from my_func_mvw.functions import find_nearest_date, read_pickle, write_pickle, temp_watertank_func, random_date\n",
    "\n",
    "plot_save=True #True False\n",
    "\n",
    "print(\"Some Version information of the imported packages\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"pickle version: {pickle.format_version}\")\n",
    "# Some Version information of the imported packages\n",
    "# pandas version: 1.2.3\n",
    "# pickle version: 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_t_logger(path_to_data_Tlogger, filename_Tlogger, data_format=\"pc_local\",channel_name=\"none\"):\n",
    "    \"\"\"\"\"\"\n",
    "    if data_format==\"CF_Card\": #auf Speicherkarte speichern: \"stand alone modus\"\n",
    "        df_Tlogger = pd.read_csv(path_to_data_Tlogger + \"\\\\\" + filename_Tlogger, sep=\";\", usecols=[1,2,3,4], names=[\"Date\",\"Time\",\"Channel1-Watertank_PT100\",\"Channel2-Air\"])\n",
    "        DateTime=df_Tlogger[\"Date\"] + \" \" + df_Tlogger[\"Time\"]\n",
    "        df_Tlogger.index =pd.to_datetime(DateTime, format=(\"%d.%m.%y  %H:%M:%S\"))\n",
    "        df_Tlogger.index.names = ['Date']\n",
    "        df_Tlogger = df_Tlogger.drop(df_Tlogger.columns[0:2],axis=1) # drop the two columns which are now used for index\n",
    "\n",
    "    if data_format == \"pc_local\": #direkt auf pc speichern\n",
    "        #channel 2 wird im moment hier nicht eingeladen, da das datum sich teilweise um wenige sekunden unterscheided\n",
    "        #müsste beide indices noch anpassen so das sie zusammenpassen\n",
    "        def import_helper(filename,channel_name):\n",
    "            df_Tlogger = pd.read_csv(\n",
    "                filename, sep=\"\\s+|\\t| \", usecols=[0,1,2],\n",
    "                names=[\"Date\",\"Time\",channel_name], engine=\"python\")\n",
    "            DateTime=df_Tlogger[\"Date\"] + \" \" + df_Tlogger[\"Time\"]\n",
    "            df_Tlogger.index =pd.to_datetime(DateTime, format=(\"%d.%m.%Y %H:%M:%S\"))\n",
    "            df_Tlogger.index.names = ['Date']\n",
    "            df_Tlogger = df_Tlogger.drop(df_Tlogger.columns[0:2],axis=1) # drop the two columns which are now used for index\n",
    "            return df_Tlogger\n",
    "\n",
    "        df_Tlogger = import_helper(path_to_data_Tlogger + \"\\\\\" + filename_Tlogger,channel_name)\n",
    "        \n",
    "    return df_Tlogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1-Watertank_PT100</th>\n",
       "      <th>Channel2-Air</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:12:40</th>\n",
       "      <td>24.948476</td>\n",
       "      <td>25.819904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:13:40</th>\n",
       "      <td>25.026853</td>\n",
       "      <td>25.443956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:14:40</th>\n",
       "      <td>24.888809</td>\n",
       "      <td>25.165027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:15:40</th>\n",
       "      <td>25.011371</td>\n",
       "      <td>24.926351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:16:40</th>\n",
       "      <td>25.014596</td>\n",
       "      <td>24.753213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:34:54</th>\n",
       "      <td>22.817576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:37:54</th>\n",
       "      <td>22.811149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:40:54</th>\n",
       "      <td>22.811149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:43:54</th>\n",
       "      <td>22.830430</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:46:54</th>\n",
       "      <td>22.772589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21766 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Channel1-Watertank_PT100  Channel2-Air\n",
       "Date                                                       \n",
       "2021-06-07 12:12:40                 24.948476     25.819904\n",
       "2021-06-07 12:13:40                 25.026853     25.443956\n",
       "2021-06-07 12:14:40                 24.888809     25.165027\n",
       "2021-06-07 12:15:40                 25.011371     24.926351\n",
       "2021-06-07 12:16:40                 25.014596     24.753213\n",
       "...                                       ...           ...\n",
       "2021-08-02 11:34:54                 22.817576           NaN\n",
       "2021-08-02 11:37:54                 22.811149           NaN\n",
       "2021-08-02 11:40:54                 22.811149           NaN\n",
       "2021-08-02 11:43:54                 22.830430           NaN\n",
       "2021-08-02 11:46:54                 22.772589           NaN\n",
       "\n",
       "[21766 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data T-logger\n",
    "path_to_data_Tlogger = r\"..\\Alsdorf\\Daten\\T-logger\"\n",
    "# PT100 sensors\n",
    "# channel1 in watertank PT100\n",
    "# channel2 air PT1000\n",
    "filename_Tlogger_0 = \"20210608\\_adc_START_07.06.2021-12_11_40_daten.txt\" \n",
    "filename_Tlogger_1 = \"20210702\\_adc_START_08.06.2021-13_21_47_daten.txt\"\n",
    "filename_Tlogger_2 = \"20210708\\_adc_START_02.07.2021-12_36_14_daten.txt\"\n",
    "filenames_CFCard = [filename_Tlogger_0,filename_Tlogger_1,filename_Tlogger_2]\n",
    "# new dataformat\n",
    "filename_Tlogger_3 = \"20210722\\kanal_1.txt\"\n",
    "filename_Tlogger_4 = \"20210727\\kanal_1.txt\"\n",
    "filename_Tlogger_5 = \"20210802\\kanal_1.txt\"\n",
    "filenames_pc = [filename_Tlogger_3,filename_Tlogger_4,filename_Tlogger_5]\n",
    "\n",
    "df_Tlogger_PT100_part={}\n",
    "for filename in filenames_CFCard:\n",
    "    df_Tlogger_PT100_part[filename] = import_t_logger(path_to_data_Tlogger, filename, data_format=\"CF_Card\")\n",
    "for filename in filenames_pc:\n",
    "    df_Tlogger_PT100_part[filename] = import_t_logger(path_to_data_Tlogger, filename, channel_name = \"Channel1-Watertank_PT100\")\n",
    "\n",
    "# concat all dataframes\n",
    "df_Tlogger_PT100 = df_Tlogger_PT100_part[filenames_CFCard[0]]\n",
    "for dataframe in list(df_Tlogger_PT100_part.keys())[1:]:\n",
    "    df_Tlogger_PT100 = pd.concat([df_Tlogger_PT100,df_Tlogger_PT100_part[dataframe]])\n",
    "df_Tlogger_PT100.sort_index(inplace=True)\n",
    "\n",
    "df_Tlogger_PT100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PT1000 sensors\n",
    "# two new sensors, both are now in watertank, I create new dataframe\n",
    "######## Add new, when downloading new data from t_logger###################\n",
    "filename_Tlogger_5_1 = \"20210809\\kanal_1.txt\"\n",
    "filename_Tlogger_5_2 = \"20210809\\kanal_2.txt\"\n",
    "filename_Tlogger_6_1 = \"20210917\\kanal_1.txt\"\n",
    "filename_Tlogger_6_2 = \"20210917\\kanal_2.txt\"\n",
    "filenames_pc_1 = [filename_Tlogger_5_1,filename_Tlogger_6_1]\n",
    "filenames_pc_2 = [filename_Tlogger_5_2,filename_Tlogger_6_2]\n",
    "##########################################################################\n",
    "\n",
    "#import data\n",
    "df_Tlogger_PT1000_part={}\n",
    "for filename in filenames_pc_1:\n",
    "    df_Tlogger_PT1000_part[filename] = import_t_logger(path_to_data_Tlogger, filename, channel_name = \"Channel1_PT1000\")\n",
    "for filename in filenames_pc_2:\n",
    "    df_Tlogger_PT1000_part[filename] = import_t_logger(path_to_data_Tlogger, filename, channel_name = \"Channel2_PT1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel2_PT1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:18:43</th>\n",
       "      <td>20.464840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:21:43</th>\n",
       "      <td>20.455567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:24:44</th>\n",
       "      <td>20.440113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:27:44</th>\n",
       "      <td>20.448613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:30:44</th>\n",
       "      <td>20.457113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:04:53</th>\n",
       "      <td>20.130295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:07:53</th>\n",
       "      <td>20.110983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:10:54</th>\n",
       "      <td>20.131067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:13:54</th>\n",
       "      <td>20.100940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:16:54</th>\n",
       "      <td>20.105575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21134 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Channel2_PT1000\n",
       "Date                                \n",
       "2021-08-04 11:18:43        20.464840\n",
       "2021-08-04 11:21:43        20.455567\n",
       "2021-08-04 11:24:44        20.440113\n",
       "2021-08-04 11:27:44        20.448613\n",
       "2021-08-04 11:30:44        20.457113\n",
       "...                              ...\n",
       "2021-09-17 12:04:53        20.130295\n",
       "2021-09-17 12:07:53        20.110983\n",
       "2021-09-17 12:10:54        20.131067\n",
       "2021-09-17 12:13:54        20.100940\n",
       "2021-09-17 12:16:54        20.105575\n",
       "\n",
       "[21134 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_Tlogger_PT1000_part[\"20210917\\kanal_2.txt\"],df_Tlogger_PT1000_part[\"20210809\\kanal_2.txt\"]]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timediff larger than 15s 4 times\n"
     ]
    }
   ],
   "source": [
    "# Only relevant for data_format = \"pc_local\"\n",
    "# these chanels can have different indexes, with a very small difference\n",
    "# I will make them more simialr to avoid a lot of nan numbers in the dataframe\n",
    "\n",
    "count_files=0 # for finding the corresponding data of kanal 1\n",
    "for part_name_kanal2 in filenames_pc_2:\n",
    "    new_index = []\n",
    "    counter = 0\n",
    "    for i in range(len(df_Tlogger_PT1000_part[part_name_kanal2].index)): # go through all dates\n",
    "        date = df_Tlogger_PT1000_part[part_name_kanal2].index[i]\n",
    "        # find date_name in other channel, which is closest to the date of channel 2\n",
    "        date_name, date_iloc = find_nearest_date(date, df_Tlogger_PT1000_part[filenames_pc_1[count_files]].index)\n",
    "        timediff = pd.to_datetime(date_name) - date\n",
    "        if timediff < timedelta(seconds = 0): # make timediff positive, if its negative\n",
    "            timediff = timediff * -1\n",
    "\n",
    "        if timediff < timedelta(seconds = 15):\n",
    "            # temp_val = df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'].values[i]\n",
    "            # dataframe = pd.DataFrame(columns=[\"Channel2_PT1000\"], index=[pd.to_datetime(date_name)])\n",
    "            # dataframe[\"Channel2_PT1000\"] = temp_val\n",
    "            # dataframe.index.names = [\"Date\"]\n",
    "            # df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'][\"Channel2_PT1000\"][i] = dataframe\n",
    "\n",
    "            new_index.append(pd.to_datetime(date_name))\n",
    "        else:\n",
    "            new_index.append(pd.to_datetime(date))\n",
    "            counter += 1\n",
    "    \n",
    "    df_Tlogger_PT1000_part[part_name_kanal2].index = new_index\n",
    "    df_Tlogger_PT1000_part[part_name_kanal2].index.name=\"Date\"\n",
    "\n",
    "    if counter > 0:\n",
    "        print(f\"timediff larger than 15s {counter} times\")\n",
    "    \n",
    "    count_files+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1_PT1000</th>\n",
       "      <th>Channel2_PT1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:18:42</th>\n",
       "      <td>20.453140</td>\n",
       "      <td>20.464840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:21:42</th>\n",
       "      <td>20.446965</td>\n",
       "      <td>20.455567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:24:42</th>\n",
       "      <td>20.448508</td>\n",
       "      <td>20.440113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:27:42</th>\n",
       "      <td>20.426124</td>\n",
       "      <td>20.448613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:30:42</th>\n",
       "      <td>20.433071</td>\n",
       "      <td>20.457113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:04:53</th>\n",
       "      <td>20.306490</td>\n",
       "      <td>20.130295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:07:53</th>\n",
       "      <td>20.296457</td>\n",
       "      <td>20.110983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:10:54</th>\n",
       "      <td>20.308034</td>\n",
       "      <td>20.131067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:13:54</th>\n",
       "      <td>20.302632</td>\n",
       "      <td>20.100940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17 12:16:54</th>\n",
       "      <td>20.298001</td>\n",
       "      <td>20.105575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Channel1_PT1000  Channel2_PT1000\n",
       "Date                                                 \n",
       "2021-08-04 11:18:42        20.453140        20.464840\n",
       "2021-08-04 11:21:42        20.446965        20.455567\n",
       "2021-08-04 11:24:42        20.448508        20.440113\n",
       "2021-08-04 11:27:42        20.426124        20.448613\n",
       "2021-08-04 11:30:42        20.433071        20.457113\n",
       "...                              ...              ...\n",
       "2021-09-17 12:04:53        20.306490        20.130295\n",
       "2021-09-17 12:07:53        20.296457        20.110983\n",
       "2021-09-17 12:10:54        20.308034        20.131067\n",
       "2021-09-17 12:13:54        20.302632        20.100940\n",
       "2021-09-17 12:16:54        20.298001        20.105575\n",
       "\n",
       "[23522 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concat all data\n",
    "df_Tlogger_PT1000_segments={}\n",
    "for export_segment in range(len(filenames_pc_1)):\n",
    "    df_Tlogger_PT1000_segments[export_segment]=pd.concat([df_Tlogger_PT1000_part[filenames_pc_1[export_segment]],df_Tlogger_PT1000_part[filenames_pc_2[export_segment]]], axis=1)\n",
    "\n",
    "df_Tlogger_PT1000=df_Tlogger_PT1000_segments[list(df_Tlogger_PT1000_segments.keys())[0]]\n",
    "for export_segment in df_Tlogger_PT1000_segments.keys():\n",
    "    df_Tlogger_PT1000=pd.concat([df_Tlogger_PT1000,df_Tlogger_PT1000_segments[export_segment]])\n",
    "\n",
    "df_Tlogger_PT1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from solexperts, which can be used for my calibration\n",
    "# import solexperts t_logger\n",
    "path_to_my_database = r\"..\\Alsdorf\\Daten\\my_database\"\n",
    "path_to_solexperts = path_to_my_database + \"\\Solexperts_EGRT\"\n",
    "filename = \"\\wagoTemperatur_korigiert.txt\"\n",
    "tlogger_sol = pd.read_csv(path_to_solexperts + filename, delimiter=\"\\t\", index_col=0)\n",
    "tlogger_sol.columns.names = [\"Dates\"]\n",
    "tlogger_sol.columns= pd.to_datetime(tlogger_sol.columns)\n",
    "tlogger_sol.index=[\"Watertank\"]\n",
    "tlogger_sol.index.names = [\"\"]\n",
    "tlogger_sol = tlogger_sol.T\n",
    "tlogger_sol.columns = [\"Channel1-Watertank_PT100\"] # name is does not fit to this data, just for concat woth my other dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a frame with no defined index and a value that cannot be converted to a Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ensure_valid_index\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   3291\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3292\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3293\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mis_empty_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[1;31m# gh-17261\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\construction.py\u001b[0m in \u001b[0;36mis_empty_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[0mis_list_like_without_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m     \u001b[0mis_simple_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_list_like_without_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mis_none\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_simple_empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1443\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-468c06496b2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# add PT1000 data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_Tlogger_übergang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Channel1-Watertank_PT100\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Channel2-Air\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#,\"Channel1-PT100_rolling_mean\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf_Tlogger_übergang\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Channel1-Watertank_PT100\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_Tlogger_PT1000\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Channel1_PT1000\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#df_Tlogger_übergang[\"Channel1-PT100_rolling_mean\"] = df_Tlogger_PT1000[\"Channel1-PT1000_rolling_mean\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf_Tlogger_PT100\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_Tlogger_PT100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_Tlogger_übergang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3162\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3239\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3240\u001b[0m         \"\"\"\n\u001b[1;32m-> 3241\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3242\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ensure_valid_index\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   3292\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3293\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3294\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   3295\u001b[0m                     \u001b[1;34m\"Cannot set a frame with no defined index \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3296\u001b[0m                     \u001b[1;34m\"and a value that cannot be converted to a Series\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set a frame with no defined index and a value that cannot be converted to a Series"
     ]
    }
   ],
   "source": [
    "# Übergangslösung bis PT1000 mehr Daten hat\n",
    "# correct old data\n",
    "df_Tlogger_PT100[\"Channel1-Watertank_PT100\"] = df_Tlogger_PT100[\"Channel1-Watertank_PT100\"] - 1.5\n",
    "\n",
    "# add PT1000 data\n",
    "df_Tlogger_übergang = pd.DataFrame(columns=[\"Channel1-Watertank_PT100\",\"Channel2-Air\"]) #,\"Channel1-PT100_rolling_mean\"\n",
    "df_Tlogger_übergang[\"Channel1-Watertank_PT100\"] = df_Tlogger_PT1000[\"Channel1_PT1000\"]\n",
    "#df_Tlogger_übergang[\"Channel1-PT100_rolling_mean\"] = df_Tlogger_PT1000[\"Channel1-PT1000_rolling_mean\"]\n",
    "df_Tlogger_PT100 = pd.concat([df_Tlogger_PT100, df_Tlogger_übergang]).sort_index()\n",
    "\n",
    "# add also some data from solexperts, which can be used for my calibration\n",
    "    # --> outside heating and cooling phase\n",
    "    # drop the data of our dataframe before concat, so nothing is doubled\n",
    "df_Tlogger_PT100 = pd.concat([df_Tlogger_PT100.drop(df_Tlogger_PT100[16160:16160+410].index), tlogger_sol[6000:]]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nan values in data gaps\n",
    "\n",
    "# find data gaps\n",
    "def add_nan_val_in_datagaps(df_Tlogger):\n",
    "    \"\"\"\"\"\"\n",
    "    diff = df_Tlogger.index[1:] - df_Tlogger.index[:-1]\n",
    "\n",
    "    index_datagaps=[]\n",
    "    for i in range(len(diff)):\n",
    "        bo = diff[i] > timedelta(minutes=10)\n",
    "        if bo == True:\n",
    "            index_datagaps.append(i)\n",
    "            #print(i)#;print(diff[i]) #i gibt einem die position des Datums ab welchem danach die Lücke ist\n",
    "\n",
    "    # add values in data gaps\n",
    "    n_appended_values = 0 # count the number of values I add to the dataframe\n",
    "    for index in index_datagaps:\n",
    "        index_corrected = index + n_appended_values # the dataframe gets new values in other date gaps before this one\n",
    "        # number of dates with nan, that will be added behin the index position\n",
    "        # round down and minus 1 to be sure I dont add a nan behind an existing date\n",
    "        n_nan_dates = math.floor(diff[index] / timedelta(minutes=3)) - 1\n",
    "        n_appended_values += n_nan_dates\n",
    "        list_nan = [np.nan] * n_nan_dates # list of nan\n",
    "\n",
    "        # add nan's in data gaps, by creating new datapoints with a timedifference of 3 min.\n",
    "        # create dataframe which contains the dates and nan values\n",
    "        new_val = pd.DataFrame({df_Tlogger.columns[0]: list_nan, df_Tlogger.columns[1]: list_nan}) #, df_Tlogger.columns[2]: list_nan}\n",
    "        new_val.index = [df_Tlogger.index[index_corrected] + timedelta(minutes=x*3) for x in range(1,n_nan_dates+1)]\n",
    "\n",
    "        # add nan values to Tlogger dataframe and sort it\n",
    "        df_Tlogger = df_Tlogger.append(new_val).sort_index()\n",
    "    print(f\"{n_appended_values} dates with nan have been added\")\n",
    "\n",
    "\n",
    "    # Check if everything worked correct\n",
    "    # find data gaps\n",
    "    diff = df_Tlogger.index[1:] - df_Tlogger.index[:-1]\n",
    "    for i in range(len(diff)):\n",
    "        bo = diff[i] > timedelta(minutes=10)\n",
    "        if bo == True:\n",
    "            print(\"There are still some indexes missing:\")\n",
    "            print(i);print(diff[i]);print() #i gibt einem die position des Datums ab welchem danach die Lücke ist\n",
    "    \n",
    "    return df_Tlogger\n",
    "\n",
    "print(\"df_PT100\")\n",
    "df_Tlogger_PT100 = add_nan_val_in_datagaps(df_Tlogger_PT100)\n",
    "print(\"df_PT1000\")\n",
    "df_Tlogger_PT1000 = add_nan_val_in_datagaps(df_Tlogger_PT1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Moving Avearage\n",
    "#https://towardsdatascience.com/moving-averages-in-python-16170e20f6c nice explanation\n",
    "df_Tlogger_PT100[\"Channel1-PT100_rolling_mean\"]=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].rolling(5,min_periods=3, center = True).mean() # 15 min window time\n",
    "\n",
    "df_Tlogger_PT1000[\"Channel1-PT1000_rolling_mean\"]=df_Tlogger_PT1000[\"Channel1_PT1000\"].rolling(5,min_periods=3, center = True).mean() # 15 min window time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round data to significant position\n",
    "df_Tlogger_PT100 = round(df_Tlogger_PT100,1)\n",
    "df_Tlogger_PT1000 = round(df_Tlogger_PT1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tlogger data to my_database\n",
    "path_to_my_database = r\"..\\Alsdorf\\Daten\\my_database\"\n",
    "\n",
    "df_Tlogger_PT100.to_csv(path_to_my_database + \"/t_logger_watertank/Tlogger_PT100_outdated.csv\")\n",
    "write_pickle(path_to_my_database + \"/t_logger_watertank/Tlogger_PT100_outdated\",df_Tlogger_PT100)\n",
    "\n",
    "df_Tlogger_PT1000.to_csv(path_to_my_database + \"/t_logger_watertank/Tlogger_PT1000.csv\")\n",
    "write_pickle(path_to_my_database + \"/t_logger_watertank/Tlogger_PT1000\",df_Tlogger_PT1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot T-Logger data\n",
    "number_test_watertank_function = 300 #Input\n",
    "plot_function_check = True #Input\n",
    "plot_air = True #Input\n",
    "\n",
    "watertank_T_range_min = df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].dropna().index.min()\n",
    "watertank_T_range_max = df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].dropna().index.max()\n",
    "\n",
    "# generate random dates in range of T-Logger dates\n",
    "random_date_list=[]\n",
    "for number in range(number_test_watertank_function):\n",
    "    r_date = random_date(watertank_T_range_min,watertank_T_range_max)\n",
    "    random_date_list.append(r_date)\n",
    "\n",
    "# Plot function and data, to see if it is good\n",
    "y=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].values\n",
    "x_dates=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].index\n",
    "T_avearage=temp_watertank_func(random_date_list, df_Tlogger_PT100)\n",
    "# I could also plot it like this, but this does not show, that my function works\n",
    "#plt.plot(df_Tlogger[\"Channel1-rolling_mean\"].index,df_Tlogger[\"Channel1-rolling_mean\"].values,label=\"Watertank rolling mean\")\n",
    "plt.figure(figsize=(16,5))\n",
    "if plot_air:\n",
    "    plt.plot(df_Tlogger_PT100[\"Channel2-Air\"].index,df_Tlogger_PT100[\"Channel2-Air\"].values,label=\"Air-Temp\",color=\"black\",zorder=8)\n",
    "plt.plot(x_dates,y,label=\"PT100 Watertank-Temp\",color=\"blue\",zorder=9)\n",
    "if plot_function_check:\n",
    "    plt.scatter(random_date_list,T_avearage,label=\"Watertank temperature function\\nat random positions\",color=\"green\",zorder=10)\n",
    "\n",
    "# y=df_Tlogger_PT1000[\"Channel1_PT1000\"].dropna().values\n",
    "# x_dates=df_Tlogger_PT1000[\"Channel1_PT1000\"].dropna().index\n",
    "# plt.plot(x_dates,y,label=\"PT1000-1 Watertank-Temp\",color=\"purple\")\n",
    "\n",
    "# y=df_Tlogger_PT1000[\"Channel2_PT1000\"].dropna().values\n",
    "# x_dates=df_Tlogger_PT1000[\"Channel2_PT1000\"].dropna().index\n",
    "# plt.plot(x_dates,y,label=\"PT1000-2 Watertank-Temp\",color=\"blue\")\n",
    "\n",
    "# y=df_Tlogger[\"Channel1-rolling_mean\"].values\n",
    "# x_dates=df_Tlogger[\"Channel1-rolling_mean\"].index\n",
    "# plt.scatter(x_dates,y,label=\"Watertank-Temp rolling_mean\",color=\"red\",zorder=10)\n",
    "\n",
    "plt.ylabel(\"T [°C]\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.title(\"T-Logger\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Wassertank am 07.06.2021 aufgefüllt, mit kälteren Wasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,1,figsize=(16,10))\n",
    "ymin=19; ymax=29\n",
    "dftlog=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"]\n",
    "dftlog_rolling=df_Tlogger_PT100[\"Channel1-PT100_rolling_mean\"]\n",
    "dftlog_sol=tlogger_sol[\"Channel1-Watertank_PT100\"][100:] #skip first ones because they are very high\n",
    "# are not perfectly chosen, but due to data gaps its looks ok\n",
    "border_temp1=20000\n",
    "border_temp2=26000\n",
    "border_temp3=32500\n",
    "\n",
    "axs[0].set_title(\"Water Tank Temperature\", fontsize=\"13\")\n",
    "axs[0].plot(dftlog[:border_temp1].index,dftlog[:border_temp1].values + 1.5,color=\"grey\", label=\"PT100 original measured\")\n",
    "axs[0].plot(dftlog[:border_temp1].index,dftlog[:border_temp1].values,color=\"red\", label=\"PT100 manually corrected\")\n",
    "\n",
    "axs[0].plot(dftlog[border_temp2:border_temp3].index,dftlog[border_temp2:border_temp3].values +1.5,color=\"grey\")\n",
    "axs[0].plot(dftlog[border_temp2:border_temp3].index,dftlog[border_temp2:border_temp3].values,color=\"red\")\n",
    "\n",
    "axs[0].plot(dftlog_sol.index,dftlog_sol.values, color=\"black\", label=\"Solexperts-Sensor\")\n",
    "\n",
    "axs[0].plot(dftlog[border_temp3:].index,dftlog[border_temp3:].values,color=\"blue\", label=\"PT1000\")\n",
    "\n",
    "\n",
    "axs[1].set_title(\"Data Used For Calibration - Rolling Mean\", fontsize=\"13\")\n",
    "axs[1].plot(dftlog_rolling[:border_temp1].index,dftlog_rolling[:border_temp1].values,color=\"red\", label=\"PT100 manually corrected\")\n",
    "\n",
    "axs[1].plot(dftlog_rolling[border_temp2:border_temp3].index,dftlog_rolling[border_temp2:border_temp3].values,color=\"red\")\n",
    "\n",
    "axs[1].plot(dftlog_rolling[border_temp1:border_temp2].index,dftlog_rolling[border_temp1:border_temp2].values,color=\"black\", label=\"Solexperts-Sensor\")\n",
    "\n",
    "axs[1].plot(dftlog_rolling[border_temp3:].index,dftlog_rolling[border_temp3:].values,color=\"blue\", label=\"PT1000\")\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "    ax.set_ylabel(\"Temperature [°C]\")\n",
    "    #ax.set_xlabel(\"Date\")\n",
    "    legend = ax.legend(fontsize=11, title_fontsize=12, frameon=True, loc=\"upper right\")\n",
    "    legend.get_frame().set_alpha(0.7) #not supported with eps\n",
    "    legend.get_frame().set_facecolor(\"white\")\n",
    "\n",
    "if plot_save:\n",
    "    plt.savefig(r\"..\\Masterthesis_tex\\figs\\chap4\\watertank_temperature.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "    plt.savefig(\"pictures\\watertank_temperature.png\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tlogger_sol[\"Channel1-Watertank_PT100\"][3500:4980])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlogger_sol[\"Channel1-Watertank_PT100\"].index[4980] - tlogger_sol[\"Channel1-Watertank_PT100\"].index[3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
