{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd029bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Compare data\n",
    "used for files in `Masterarbeit\\Alsdorf\\Daten`\n",
    "\n",
    "compare the data created by merge_files.ipynb and exported from Charon4 with the data from sciebo repository eduardschacht.\n",
    "The data is from 2019.\n",
    "\n",
    "In the comparisson an error=0.011 is allowed. The data from sciebo is rounded to the second digit. \n",
    "\n",
    "I copied the data created with merge_files.ipynb to `Daten_Mario_oder_alt\\compiled_data_desktop-folder`. The data from sciebo is in `Betrieb_2019`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# User-Defined-Functions for Data import\n",
    "def import_data_Daniel(path):\n",
    "    \"\"\"import for data from sciebo Eduardschacht repository\"\"\"\n",
    "    df = pd.read_csv(path,delimiter = ',',index_col=0, header=7 )\n",
    "    df = df.drop(df.columns[[0,1]], axis=1)\n",
    "    df.index=pd.to_datetime(df.index, dayfirst = True).tz_localize(None)\n",
    "    df.index=df.index.astype(str) # same column format as other dataframe\n",
    "    df=df.transpose() # for having the same format as the other dataframe\n",
    "    df.index=df.index.astype(float) #make depth to floats\n",
    "    df.index.names = ['Depth [m]']\n",
    "    return df\n",
    "\n",
    "def import_merge_files_output(path):\n",
    "    \"\"\"imports the data created with the merge_files script\"\"\"\n",
    "    df2=pd.read_csv(path,index_col=0)\n",
    "    df2 = df2.round(decimals=2) #same number of decimal places as other dataframe\n",
    "    return df2\n",
    "\n",
    "def put_files_in_dataframe(list_of_paths,import_function):\n",
    "    \"\"\"\n",
    "    merges files in one dataframe using an import_function (user-defined-functiom) in a list_of paths (list)\n",
    "    \"\"\"\n",
    "    # Import Data\n",
    "    dic={}\n",
    "    for path in list_of_paths:\n",
    "        dic[path]=import_function(path)\n",
    "\n",
    "    # Merge files into one dataframe\n",
    "    dic_concat=pd.concat(dic.values(),axis=1)\n",
    "    \n",
    "    return dic_concat\n",
    "\n",
    "def get_abspath(basepath):\n",
    "    \"\"\"Get the files you need to import into your script with Path\n",
    "    Returns a list of all filepaths (or folderpaths) of the files (or folders) in a repository.\n",
    "    The repository is given this function with basepath.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    basepath = glob.glob(basepath)\n",
    "    for entry in basepath:\n",
    "        df_list.append(entry)\n",
    "    return (df_list)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diese Zelle könnte man noch mit einem dictionary optimieren, wird dann aber evntl. unübersichtlicher?\n",
    "\n",
    "# Import Data\n",
    "#at the moment the data wont be sorted after import\n",
    "\n",
    "## Data Paths\n",
    "paths_to_working_dir=r\"..\\Alsdorf\\Daten\"\n",
    "### daniel\n",
    "daniel_ch1_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch1\\*\")\n",
    "daniel_ch2_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch2\\*\")\n",
    "daniel_ch3_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch3\\*\")\n",
    "daniel_ch4_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch4\\*\")\n",
    "### merge (other date period than daniel)\n",
    "merge_ch1_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch1_2019_*\")\n",
    "merge_ch2_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch2_2019_*\")\n",
    "merge_ch3_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch3_2019_*\")\n",
    "merge_ch4_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch4_2019_*\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Wall time: 1min 2s\n",
    "## Import files and merge them in one dataframe\n",
    "### Channel 1\n",
    "daniel_ch1=put_files_in_dataframe(daniel_ch1_paths,import_data_Daniel)\n",
    "merge_ch1=put_files_in_dataframe(merge_ch1_paths,import_merge_files_output)\n",
    "### Channel 2\n",
    "daniel_ch2=put_files_in_dataframe(daniel_ch2_paths,import_data_Daniel)\n",
    "merge_ch2=put_files_in_dataframe(merge_ch2_paths,import_merge_files_output)\n",
    "### Channel 3\n",
    "daniel_ch3=put_files_in_dataframe(daniel_ch3_paths,import_data_Daniel)\n",
    "merge_ch3=put_files_in_dataframe(merge_ch3_paths,import_merge_files_output)\n",
    "### Channel 4\n",
    "daniel_ch4=put_files_in_dataframe(daniel_ch4_paths,import_data_Daniel)\n",
    "merge_ch4=put_files_in_dataframe(merge_ch4_paths,import_merge_files_output)\n",
    "\n",
    "\n",
    "# Drop columns which are in merge but not in daniel\n",
    "# These are detected during the comparisson of the dataframes.\n",
    "# This results in an error of the code, so these are excluded. \n",
    "# It also means, that these are new data!!!\n",
    "merge_ch3=merge_ch3.drop(columns=['2019-03-31 21:59:31'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daniel_ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Defined-Functions for comparing the dataframes\n",
    "\n",
    "def number_of_False_in_List(list_bool):\n",
    "    \"\"\"\n",
    "    input: List of True and False\n",
    "    returns: list of Index for all False entries\n",
    "    \"\"\"\n",
    "    n_True=sum(list_bool) #number of Trues\n",
    "    n_False=len(list_bool)-n_True #number of False\n",
    "    \n",
    "    # create index of all False entries\n",
    "    index_False=[]\n",
    "    for i, j in enumerate(list_bool):\n",
    "        if j == False:\n",
    "            index_False.append(i)\n",
    "    return index_False, n_False\n",
    "\n",
    "\n",
    "def compare_dataframes(df1_merge=merge_ch1,df2_daniel=daniel_ch1):\n",
    "    \"\"\"\n",
    "    compare both dataframes if the values are equal\n",
    "\n",
    "    df1_merge has to the merge dataframe, because it contains less data\n",
    "    merge has less columns (less data), so it will be compared if all merge data is also in daniel data\n",
    "    and if these are equal\n",
    "    \"\"\"\n",
    "    # the print assumes that the data is sorted, which is not necessary the case\n",
    "    #print(\"compare %.20s to %.20s of from the merged data with daniels data from sciebo\"%(df1_merge.columns.min(), df1_merge.columns.max()))\n",
    "\n",
    "    # check if all columns of merge are also in daniel dataframe\n",
    "    #list of columns names which are in both\n",
    "    columns_both=[i for i in df1_merge.columns if i in df2_daniel.columns]\n",
    "    if set(df1_merge.columns).issubset(df2_daniel.columns):\n",
    "        print(\"merge columns are a subset of daniel columns\")\n",
    "    else: #if this happens the following code does not work\n",
    "        print(\"the following columns of merge are not in daniel\")\n",
    "        columns_not_in_daniel = [x for x in df1_merge.columns if x not in df2_daniel.columns]\n",
    "        print(columns_not_in_daniel)\n",
    "    \n",
    "    # compare all values of the columns\n",
    "    all_checks=[]\n",
    "    for column in df1_merge.keys():\n",
    "        #list of True False: each value of one columns is compared, based on the index, \n",
    "        #with the other dataframe column\n",
    "        check=df1_merge[column]==df2_daniel[column]\n",
    "        a=all(check) #returns True if all elements are True\n",
    "        all_checks.append(a) # contains one entry for each column\n",
    "    print(\"all columns are equal: %.10s\"%all(all_checks))\n",
    "\n",
    "    if all(all_checks)==False: #some values are not equal\n",
    "        #Here the difference is calculated, when the difference is smaller than error it is ok.\n",
    "        #an error of 0.01 could be somehow due to rounding. Did not further investigate why this difference exists\n",
    "        #I think that is not very important\n",
    "        error=0.011\n",
    "\n",
    "        index_false, n_False=number_of_False_in_List(all_checks) # index of columns which are not completly equal\n",
    "        print(\"number of False in list: %.0f\"%n_False)\n",
    "\n",
    "        print(\"check all False values and calculate difference\")\n",
    "        counter_exceeded_error=0\n",
    "        for column_number in index_false: # go through all columns where False values are\n",
    "            # check where in the column are the False values\n",
    "            check=df2_daniel[df1_merge.keys()[column_number]]==df1_merge[df1_merge.keys()[column_number]]\n",
    "            depth_false, n_False=number_of_False_in_List(check) # extract depth for all False values in this column\n",
    "            #print(\"number of False in list: %.0f\"%n_False)\n",
    "\n",
    "            for depth in depth_false: # for each false depth, calculate the difference of the Temp.\n",
    "                diff=df1_merge[df1_merge.keys()[column_number]][depth] - df2_daniel[df1_merge.keys()[column_number]][depth]\n",
    "\n",
    "                if diff > error or diff < -1*error:\n",
    "                    print(\"-----WARNING-----\")\n",
    "                    print(\"in %.0f at length %.0f the difference is larger than %.2f\"%(column_number,depth,error))\n",
    "                    print(diff)\n",
    "                    print()\n",
    "                    counter_exceeded_error+=1\n",
    "        if counter_exceeded_error==0:\n",
    "            print(\"all differences are in the range of %.4f\"%error)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CH1\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: False\n",
      "number of False in list: 48\n",
      "check all False values and calculate difference\n",
      "all differences are in the range of 0.0110\n",
      "\n",
      "CH2\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: False\n",
      "number of False in list: 28\n",
      "check all False values and calculate difference\n",
      "all differences are in the range of 0.0110\n",
      "\n",
      "CH3\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: False\n",
      "number of False in list: 36\n",
      "check all False values and calculate difference\n",
      "all differences are in the range of 0.0110\n",
      "\n",
      "CH4\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: False\n",
      "number of False in list: 20\n",
      "check all False values and calculate difference\n",
      "all differences are in the range of 0.0110\n",
      "\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Wall time: 32 s\n",
    "\n",
    "# Compare dataframes\n",
    "print(\"CH1\")\n",
    "compare_dataframes(merge_ch1,daniel_ch1)\n",
    "print(\"CH2\")\n",
    "compare_dataframes(merge_ch2,daniel_ch2)\n",
    "print(\"CH3\")\n",
    "compare_dataframes(merge_ch3,daniel_ch3)\n",
    "print(\"CH4\")\n",
    "compare_dataframes(merge_ch4,daniel_ch4)"
   ]
  },
  {
   "source": [
    "# Compare daniel Data with Charon4 data exported by me"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_charon4_txt(filename):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    #path_to_controller=r\"..\\Alsdorf\\Daten\\export\\charon4_export_as_txt\"\n",
    "    save_to = path_to_controller + \"\\..\\..\\my_database\"\n",
    "    df=pd.read_csv(save_to + filename, index_col=0)\n",
    "    return df\n",
    "\n",
    "# temp_ch1_2021.csv temp_ch2_2021.csv temp_ch3_2021.csv temp_ch4_2021.csv\n",
    "# temp_ch1_2020.csv temp_ch2_2020.csv temp_ch3_2020.csv temp_ch4_2020.csv\n",
    "# temp_ch1_2019.csv temp_ch2_2019.csv temp_ch3_2019.csv temp_ch4_2019.csv\n",
    "#filename=f\"\\\\temp_ch2_2020.csv\" \n",
    "data_charon4_2019={}\n",
    "for channel in [\"1\",\"2\",\"3\",\"4\"]:\n",
    "    data_charon4_2019[channel]=import_charon4_txt(f\"\\\\temp_ch{channel}_2019.csv\")\n",
    "    # make it the same format as the other dataframes in this notebook\n",
    "    data_charon4_2019[channel].index.names = ['']\n",
    "    data_charon4_2019[channel]=data_charon4_2019[channel].transpose()\n",
    "    data_charon4_2019[channel].index.names = ['Depth [m]']\n",
    "    data_charon4_2019[channel].index=data_charon4_2019[channel].index.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ch1\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: True\n",
      "\n",
      "ch2\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: True\n",
      "\n",
      "ch3\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: False\n",
      "number of False in list: 1808\n",
      "check all False values and calculate difference\n",
      "all differences are in the range of 0.0110\n",
      "\n",
      "\n",
      "ch4\n",
      "merge columns are a subset of daniel columns\n",
      "all columns are equal: True\n"
     ]
    }
   ],
   "source": [
    "# now it is smaller than daniel data, and all of this data should be in daniel data\n",
    "subset_data_charon4_2019_ch1=data_charon4_2019[\"1\"][data_charon4_2019[\"1\"].columns[5000:]]\n",
    "subset_data_charon4_2019_ch2=data_charon4_2019[\"2\"][data_charon4_2019[\"2\"].columns[5000:]]\n",
    "subset_data_charon4_2019_ch3=data_charon4_2019[\"3\"][data_charon4_2019[\"3\"].columns[5000:]]\n",
    "subset_data_charon4_2019_ch4=data_charon4_2019[\"4\"][data_charon4_2019[\"4\"].columns[5000:]]\n",
    "\n",
    "print(\"ch1\")\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch1,df2_daniel=daniel_ch1)\n",
    "print();print(\"ch2\")\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch2,df2_daniel=daniel_ch2)\n",
    "print();print(\"ch3\")\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch3,df2_daniel=daniel_ch3)\n",
    "print();print(\"ch4\")\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch4,df2_daniel=daniel_ch4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}