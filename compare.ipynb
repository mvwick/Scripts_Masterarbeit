{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd029bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compare data\r\n",
    "compare the data created by `merge_files.ipynb` and exported from Charon4 with the data from sciebo repository eduardschacht.\r\n",
    "The data is from 2019.\r\n",
    "\r\n",
    "In the comparison an error=0.011 is allowed. The data from sciebo is rounded to the second digit. \r\n",
    "\r\n",
    "I copied the data created with `merge_files.ipynb` to `sciebo\\DTS Data\\Alsdorf\\Daten\\DTS_unprocessedDaten_Mario_oder_alt\\compiled_data_desktop-folder`.\r\n",
    "\r\n",
    "## Outdated\r\n",
    "this script is not needed anymore because the data in `Daten_Mario_oder_alt` is the same as in the used database.\r\n",
    "I keep it here if someone wants to check that again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Imports\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "\r\n",
    "from my_func_mvw.functions import get_abspath\r\n",
    "\r\n",
    "# User-Defined-Functions for Data import\r\n",
    "def import_data_Daniel(path):\r\n",
    "    \"\"\"import for data from sciebo Eduardschacht repository\"\"\"\r\n",
    "    df = pd.read_csv(path,delimiter = ',',index_col=0, header=7 )\r\n",
    "    df = df.drop(df.columns[[0,1]], axis=1)\r\n",
    "    df.index=pd.to_datetime(df.index, dayfirst = True).tz_localize(None)\r\n",
    "    df.index=df.index.astype(str) # same column format as other dataframe\r\n",
    "    df=df.transpose() # for having the same format as the other dataframe\r\n",
    "    df.index=df.index.astype(float) #make depth to floats\r\n",
    "    df.index.names = ['Depth [m]']\r\n",
    "    return df\r\n",
    "\r\n",
    "def import_merge_files_output(path):\r\n",
    "    \"\"\"imports the data created with the merge_files script\"\"\"\r\n",
    "    df2=pd.read_csv(path,index_col=0)\r\n",
    "    df2 = df2.round(decimals=2) #same number of decimal places as other dataframe\r\n",
    "    return df2\r\n",
    "\r\n",
    "def put_files_in_dataframe(list_of_paths,import_function):\r\n",
    "    \"\"\"\r\n",
    "    merges files in one dataframe using an import_function (user-defined-functiom) in a list_of paths (list)\r\n",
    "    \"\"\"\r\n",
    "    # Import Data\r\n",
    "    dic={}\r\n",
    "    for path in list_of_paths:\r\n",
    "        dic[path]=import_function(path)\r\n",
    "\r\n",
    "    # Merge files into one dataframe\r\n",
    "    dic_concat=pd.concat(dic.values(),axis=1)\r\n",
    "    \r\n",
    "    return dic_concat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import Data\r\n",
    "\r\n",
    "## Data Paths\r\n",
    "paths_to_working_dir=r\"..\\Alsdorf\\Daten\\unprocessed\"\r\n",
    "### daniel\r\n",
    "daniel_ch1_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch1\\*\")\r\n",
    "daniel_ch2_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch2\\*\")\r\n",
    "daniel_ch3_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch3\\*\")\r\n",
    "daniel_ch4_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch4\\*\")\r\n",
    "### merge (other date period than daniel)\r\n",
    "merge_ch1_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch1_2019_*\")\r\n",
    "merge_ch2_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch2_2019_*\")\r\n",
    "merge_ch3_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch3_2019_*\")\r\n",
    "merge_ch4_paths=get_abspath(paths_to_working_dir + r\"\\Daten_Mario_oder_alt\\compiled_data_desktop-folder\\temp_ch4_2019_*\") \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Import files and merge them in one dataframe\r\n",
    "### Channel 1\r\n",
    "daniel_ch1=put_files_in_dataframe(daniel_ch1_paths,import_data_Daniel)\r\n",
    "merge_ch1=put_files_in_dataframe(merge_ch1_paths,import_merge_files_output)\r\n",
    "### Channel 2\r\n",
    "daniel_ch2=put_files_in_dataframe(daniel_ch2_paths,import_data_Daniel)\r\n",
    "merge_ch2=put_files_in_dataframe(merge_ch2_paths,import_merge_files_output)\r\n",
    "### Channel 3\r\n",
    "daniel_ch3=put_files_in_dataframe(daniel_ch3_paths,import_data_Daniel)\r\n",
    "merge_ch3=put_files_in_dataframe(merge_ch3_paths,import_merge_files_output)\r\n",
    "### Channel 4\r\n",
    "daniel_ch4=put_files_in_dataframe(daniel_ch4_paths,import_data_Daniel)\r\n",
    "merge_ch4=put_files_in_dataframe(merge_ch4_paths,import_merge_files_output)\r\n",
    "\r\n",
    "\r\n",
    "# Drop columns which are in merge but not in daniel\r\n",
    "# These are detected during the comparisson of the dataframes.\r\n",
    "# This results in an error of the code, so these are excluded. \r\n",
    "# It also means, that these are new data!!!\r\n",
    "merge_ch3=merge_ch3.drop(columns=['2019-03-31 21:59:31'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#daniel_ch1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#merge_ch1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# User-Defined-Functions for comparing the dataframes\r\n",
    "\r\n",
    "def number_of_False_in_List(list_bool):\r\n",
    "    \"\"\"\r\n",
    "    input: List of True and False\r\n",
    "    returns: list of Index for all False entries\r\n",
    "    \"\"\"\r\n",
    "    n_True=sum(list_bool) #number of Trues\r\n",
    "    n_False=len(list_bool)-n_True #number of False\r\n",
    "    \r\n",
    "    # create index of all False entries\r\n",
    "    index_False=[]\r\n",
    "    for i, j in enumerate(list_bool):\r\n",
    "        if j == False:\r\n",
    "            index_False.append(i)\r\n",
    "    return index_False, n_False\r\n",
    "\r\n",
    "\r\n",
    "def compare_dataframes(df1_merge=merge_ch1,df2_daniel=daniel_ch1):\r\n",
    "    \"\"\"\r\n",
    "    compare both dataframes if the values are equal\r\n",
    "\r\n",
    "    df1_merge has to the merge dataframe, because it contains less data\r\n",
    "    merge has less columns (less data), so it will be compared if all merge data is also in daniel data\r\n",
    "    and if these are equal\r\n",
    "    \"\"\"\r\n",
    "    # the print assumes that the data is sorted, which is not necessary the case\r\n",
    "    #print(\"compare %.20s to %.20s of from the merged data with daniels data from sciebo\"%(df1_merge.columns.min(), df1_merge.columns.max()))\r\n",
    "\r\n",
    "    # check if all columns of merge are also in daniel dataframe\r\n",
    "    #list of columns names which are in both\r\n",
    "    columns_both=[i for i in df1_merge.columns if i in df2_daniel.columns]\r\n",
    "    if set(df1_merge.columns).issubset(df2_daniel.columns):\r\n",
    "        print(\"merge columns are a subset of daniel columns\")\r\n",
    "    else: #if this happens the following code does not work\r\n",
    "        print(\"the following columns of merge are not in daniel\")\r\n",
    "        columns_not_in_daniel = [x for x in df1_merge.columns if x not in df2_daniel.columns]\r\n",
    "        print(columns_not_in_daniel)\r\n",
    "    \r\n",
    "    # compare all values of the columns\r\n",
    "    all_checks=[]\r\n",
    "    for column in df1_merge.keys():\r\n",
    "        #list of True False: each value of one columns is compared, based on the index, \r\n",
    "        #with the other dataframe column\r\n",
    "        check=df1_merge[column]==df2_daniel[column]\r\n",
    "        a=all(check) #returns True if all elements are True\r\n",
    "        all_checks.append(a) # contains one entry for each column\r\n",
    "    print(\"all columns are equal: %.10s\"%all(all_checks))\r\n",
    "\r\n",
    "    if all(all_checks)==False: #some values are not equal\r\n",
    "        #Here the difference is calculated, when the difference is smaller than error it is ok.\r\n",
    "        #an error of 0.01 could be somehow due to rounding. Did not further investigate why this difference exists\r\n",
    "        #I think that is not very important\r\n",
    "        error=0.011\r\n",
    "\r\n",
    "        index_false, n_False=number_of_False_in_List(all_checks) # index of columns which are not completly equal\r\n",
    "        print(\"number of False in list: %.0f\"%n_False)\r\n",
    "\r\n",
    "        print(\"check all False values and calculate difference\")\r\n",
    "        counter_exceeded_error=0\r\n",
    "        for column_number in index_false: # go through all columns where False values are\r\n",
    "            # check where in the column are the False values\r\n",
    "            check=df2_daniel[df1_merge.keys()[column_number]]==df1_merge[df1_merge.keys()[column_number]]\r\n",
    "            depth_false, n_False=number_of_False_in_List(check) # extract depth for all False values in this column\r\n",
    "            #print(\"number of False in list: %.0f\"%n_False)\r\n",
    "\r\n",
    "            for depth in depth_false: # for each false depth, calculate the difference of the Temp.\r\n",
    "                diff=df1_merge[df1_merge.keys()[column_number]][depth] - df2_daniel[df1_merge.keys()[column_number]][depth]\r\n",
    "\r\n",
    "                if diff > error or diff < -1*error:\r\n",
    "                    print(\"-----WARNING-----\")\r\n",
    "                    print(\"in %.0f at length %.0f the difference is larger than %.2f\"%(column_number,depth,error))\r\n",
    "                    print(diff)\r\n",
    "                    print()\r\n",
    "                    counter_exceeded_error+=1\r\n",
    "        if counter_exceeded_error==0:\r\n",
    "            print(\"all differences are in the range of %.4f\"%error)\r\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compare dataframes\r\n",
    "print(\"CH1\")\r\n",
    "compare_dataframes(merge_ch1,daniel_ch1)\r\n",
    "print(\"CH2\")\r\n",
    "compare_dataframes(merge_ch2,daniel_ch2)\r\n",
    "print(\"CH3\")\r\n",
    "compare_dataframes(merge_ch3,daniel_ch3)\r\n",
    "print(\"CH4\")\r\n",
    "compare_dataframes(merge_ch4,daniel_ch4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare daniel Data with Charon4 data exported by me"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def import_charon4_txt(filename):\r\n",
    "    \"\"\"changed it druing repository structure change, not sure if it works as it expected; not tested\"\"\"\r\n",
    "    save_to = \"..\\Alsdorf\\Daten\\my_database\\csv\" # outdated\r\n",
    "    df=pd.read_csv(save_to + filename, index_col=0)\r\n",
    "    return df\r\n",
    "\r\n",
    "# temp_ch1_2021.csv temp_ch2_2021.csv temp_ch3_2021.csv temp_ch4_2021.csv\r\n",
    "# temp_ch1_2020.csv temp_ch2_2020.csv temp_ch3_2020.csv temp_ch4_2020.csv\r\n",
    "# temp_ch1_2019.csv temp_ch2_2019.csv temp_ch3_2019.csv temp_ch4_2019.csv\r\n",
    "#filename=f\"\\\\temp_ch2_2020.csv\" \r\n",
    "data_charon4_2019={}\r\n",
    "for channel in [\"1\",\"2\",\"3\",\"4\"]:\r\n",
    "    data_charon4_2019[channel]=import_charon4_txt(f\"\\\\temp_ch{channel}_2019.csv\")\r\n",
    "    # make it the same format as the other dataframes in this notebook\r\n",
    "    data_charon4_2019[channel].index.names = ['']\r\n",
    "    data_charon4_2019[channel]=data_charon4_2019[channel].transpose()\r\n",
    "    data_charon4_2019[channel].index.names = ['Depth [m]']\r\n",
    "    data_charon4_2019[channel].index=data_charon4_2019[channel].index.astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now it is smaller than daniel data, and all of this data should be in daniel data\r\n",
    "subset_data_charon4_2019_ch1=data_charon4_2019[\"1\"][data_charon4_2019[\"1\"].columns[5000:]]\r\n",
    "subset_data_charon4_2019_ch2=data_charon4_2019[\"2\"][data_charon4_2019[\"2\"].columns[5000:]]\r\n",
    "subset_data_charon4_2019_ch3=data_charon4_2019[\"3\"][data_charon4_2019[\"3\"].columns[5000:]]\r\n",
    "subset_data_charon4_2019_ch4=data_charon4_2019[\"4\"][data_charon4_2019[\"4\"].columns[5000:]]\r\n",
    "\r\n",
    "print(\"ch1\")\r\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch1,df2_daniel=daniel_ch1)\r\n",
    "print();print(\"ch2\")\r\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch2,df2_daniel=daniel_ch2)\r\n",
    "print();print(\"ch3\")\r\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch3,df2_daniel=daniel_ch3)\r\n",
    "print();print(\"ch4\")\r\n",
    "compare_dataframes(df1_merge=subset_data_charon4_2019_ch4,df2_daniel=daniel_ch4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}