{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charon4 txt to Python\n",
    "This script loads the data exported with charon4. The following screenshot shows the expected settings while exporting.\n",
    "\n",
    "2018 data are exported processed different, see `charon4_txt_to_python_2018data.ipynb`\n",
    "\n",
    "<img src=\".\\pictures\\Charon4_export_window_txt.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The results of this script are stored in `..\\Alsdorf\\Daten\\my_database`. The goal of this database is to contain all data in an easy to read format.\n",
    "\n",
    "At the moment this script always loads and saves all data, it does not check which data already is in the existing database. I did not plan yet to implement that\n",
    "\n",
    "After using this script you should run `my_database_script.ipynb`, which gives an overview of the data.\n",
    "\n",
    "## Data download in Alsdorf\n",
    "Daten sind direkt runtergeladen, wenn man ankommt. Aber noch nicht auf dem Gerät gelöscht. Dafür habe ich dann data download seperat angeklickt, so werden dann die Daten auf der Speicherkarte gelöscht. Es wird aber anscheinend nichts doppelt in die Database geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Version information of the imported packages\n",
      "pandas: 1.2.3\n",
      "pickle: 4.0\n",
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "version=\"v1_5\"\n",
    "# Changelog\n",
    "#v1_0: has a nice overviwe at the end\n",
    "#v1_1: add temp data from daniel to data_2019; length are int now; added seperate import for 2018\n",
    "#v1_2: seperated this notebook into different ones; outsourced some user-defined functions\n",
    "#v1_3: added import for multiple controller files; removed_data_for_month_avaible variable and plot, \n",
    "# removed a bug in merging daniel data in 2019\n",
    "#v1_4: added sort_index before data save (only saved data is sorted); dropped two nan rows in 2021\n",
    "# removed measurements per day plots from these script and added them to my_databse_script\n",
    "#v1_5: \n",
    "\n",
    "#???????????Charon4 does not export the last two hours, as selected? But exports 2 hours more at the beginning? \n",
    "# Time Zone recalculation within the software?\n",
    "# --> irgendwann nochmal genauer checken\n",
    "\n",
    "# hiervon werden nicht mehr alle gebraucht\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib import colors\n",
    "#from datetime import date, timedelta\n",
    "from collections import defaultdict\n",
    "#from collections import Counter\n",
    "#import matplotlib.patches as patches\n",
    "#import matplotlib.dates as mdates\n",
    "#from shutil import copy2 as copy_file\n",
    "import pickle\n",
    "\n",
    "from my_func_mvw.functions import get_abspath, read_pickle, write_pickle\n",
    "from my_func_mvw.functions_import_my_database import merge_data_year,import_my_database_pickle\n",
    "\n",
    "print(\"Some Version information of the imported packages\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"pickle: {pickle.format_version}\")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to working directory\n",
    "#mit \\[-]\\ im Pfad funktioniert die get_abspath Funktion nicht\n",
    "# add additional paths to list, if you exported new data from Charon4\n",
    "# if you have a lot of different paths to controller files, consider exporting the whole databse from Charon4 again\n",
    "# this would increase the speed of this script (having less paths to controllers)\n",
    "# but the export from Charon4 takes some time\n",
    "# selecting the same repository in Charon4 does not work. Doing that, every file is copied at the end of its counterpart.\n",
    "\n",
    "############Input###################\n",
    "path_to_controller_Alsdorf=[\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\07062021\", \n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\08062021\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\20210702\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\20210719\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\20210727\\Alsdorf\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\20210804\\Alsdorf\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\20210809\\Alsdorf\",\n",
    "    r\"..\\Alsdorf\\Daten\\Charon4\\charon4_export_as_txt\\20210917\\Alsdorf\"\n",
    "    ]\n",
    "\n",
    "path_to_controller_EONERC=[\n",
    "    r\"C:\\Users\\Mathis\\ownCloud\\my_EONERC_data\\charon4_export_as_txt\\EON ERC\"\n",
    "]\n",
    "\n",
    "data_save_csv=True # True False; data saving takes about 9 min.\n",
    "data_save_pickle=True #True False; data saving takes about 25 s.\n",
    "# 3188 is in Alsdorf; 3195 in EONERC\n",
    "controller=3188 #3188 3195\n",
    "######################################\n",
    "\n",
    "# pickle files are much faster saved and read, but need slightly more disc space\n",
    "# csv seems to be more robust to version changes of packages than pickle\n",
    "# this gives a nice short overview about different file types: \n",
    "# https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My old Inputs, dont change them. I am not sure how good they work.\n",
    "if controller==3188:\n",
    "    path_to_controller = path_to_controller_Alsdorf\n",
    "    channels=[1,2,3,4,5,6,7,8] #[1,2,3,4,5,6,7,8]\n",
    "    years=[2019,2020,2021] #[2019,2020,2021] # 2018 is imported seperate from another data format\n",
    "    months=[1,2,3,4,5,6,7,8,9,10,11,12] #[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "if controller==3195:\n",
    "    path_to_controller = path_to_controller_EONERC\n",
    "    ########################################################\n",
    "    channels=[1,2,3,4,5,6,8,9,10,11]\n",
    "    years=[2019,2020,2021]\n",
    "    months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "\n",
    "# generate all requested paths, some paths may be empty\n",
    "# see here why defaultdict is used:\n",
    "# https://stackoverflow.com/questions/44992106/how-do-i-create-a-nested-dictionary-under-a-key-that-is-yet-to-exist\n",
    "paths_for_activate=defaultdict(dict)\n",
    "for path_controller in path_to_controller:\n",
    "    for year in years:\n",
    "        for channel in channels:\n",
    "            for month in months:\n",
    "                paths_for_activate[path_controller][f\"temp_ch{channel}_year{year}_month{month}\"] = path_controller + f\"\\Controller\\{controller}\\{channel}\\Temperature Data\\{year}\\{month}\"\n",
    "\n",
    "#paths_for_activate\n",
    "\n",
    "#Idee\n",
    "# für jeden controller path seperate dic erstellen und alles einladen\n",
    "# dic danach mergen\n",
    "# darauf achten, dass doppelte Messungen am ende nicht auftauchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_temp_to_df(path):\n",
    "    \"\"\"\"\"\"\n",
    "    #print(path)\n",
    "    # error_bad_lines, needed for data of 2018, where the length of the cable changes\n",
    "    # the data is loast at the moment, by adding columns / changing the data file the data could be imported\n",
    "    # I think this data wont be important anyway\n",
    "    df = pd.read_csv(path,skiprows=7,decimal=\".\",delimiter=\"\\t\",index_col=0,error_bad_lines=False,warn_bad_lines=False)\n",
    "    df = df.drop(df.columns[0:2],axis=1)\n",
    "    df.index.names = ['Date']\n",
    "    df.index = pd.to_datetime(df.index, infer_datetime_format=True)\n",
    "    df.rename(columns=lambda x: int(float(x)), inplace=True)\n",
    "    df.columns.names=[\"Length [m]\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "# maybe its best to store not everything in one variable, not sure if thats True. \n",
    "\n",
    "def import_path_for_activate(paths_for_activate):\n",
    "    \"\"\"imports all files listed in one path_for_activate dic\n",
    "    paths to empty repositories are not used\n",
    "    \"\"\"\n",
    "\n",
    "    data_2019=defaultdict(dict)\n",
    "    data_2020=defaultdict(dict)\n",
    "    data_2021=defaultdict(dict)\n",
    "\n",
    "    for name in paths_for_activate.keys(): # loop for every channel of every month\n",
    "        my_path=paths_for_activate[name] + \"\\*\"\n",
    "        my_path_to_file=get_abspath(my_path)\n",
    "\n",
    "        # get info about data from name\n",
    "        partition1=name.partition(\"_year\")[-1].partition(\"_month\")\n",
    "        year=partition1[0]\n",
    "        month=partition1[-1]\n",
    "        channelnumber=name.partition(\"_year\")[0].partition(\"temp_ch\")[-1]\n",
    "\n",
    "        # check if the month folder contains more than one file \n",
    "        if len(my_path_to_file) > 1:\n",
    "            if name != \"temp_ch2_year2019_month7\":\n",
    "                print(\"one months contains more than one file, script should be checked! Only encountered this ones\")\n",
    "                print(name)\n",
    "            # adaption of script\n",
    "            # Import Data, one option\n",
    "            one_file=import_temp_to_df(my_path_to_file[0])\n",
    "            for i in range(1,len(my_path_to_file)):\n",
    "                additional_files=import_temp_to_df(my_path_to_file[i])\n",
    "                #display(additional_files)\n",
    "                #display(one_file)\n",
    "                one_file=pd.concat([one_file,additional_files],axis=0)\n",
    "                #display(one_file)\n",
    "            \n",
    "            ## Store Data in dic - best would be putting this in a user-defined-function\n",
    "            # or creating an if with >=1\n",
    "            \n",
    "            if year == \"2019\":\n",
    "                data_2019[channelnumber][month] = one_file\n",
    "\n",
    "            if year == \"2020\":\n",
    "                data_2020[channelnumber][month] = one_file\n",
    "\n",
    "            if year == \"2021\":\n",
    "                data_2021[channelnumber][month] = one_file\n",
    "        \n",
    "        #if len(my_path_to_file) == 0: # wenn pfad zu keinem file führt dann wird leere Liste returned\n",
    "        \n",
    "        if len(my_path_to_file) == 1:\n",
    "            \n",
    "            # Import Data, second option\n",
    "            one_file=import_temp_to_df(my_path_to_file[0])\n",
    "\n",
    "            ## Store Data in dic\n",
    "            \n",
    "            if year == \"2019\":\n",
    "                data_2019[channelnumber][month] = one_file\n",
    "\n",
    "            if year == \"2020\":\n",
    "                data_2020[channelnumber][month] = one_file\n",
    "\n",
    "            if year == \"2021\":\n",
    "                data_2021[channelnumber][month] = one_file\n",
    "\n",
    "    return data_2019, data_2020, data_2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Wall time: 130s\n",
    "# Import data\n",
    "\n",
    "n_controller_files=len(path_to_controller)\n",
    "data_2019_parts=[]\n",
    "data_2020_parts=[]\n",
    "data_2021_parts=[]\n",
    "counter=0\n",
    "for path in path_to_controller:\n",
    "    data_2019_part, data_2020_part, data_2021_part = import_path_for_activate(paths_for_activate[path])\n",
    "\n",
    "    data_2019_parts.append(data_2019_part)\n",
    "    data_2020_parts.append(data_2020_part)\n",
    "    data_2021_parts.append(data_2021_part)\n",
    "\n",
    "    counter+=1\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dics_data(data_A, data_B):\n",
    "    \"\"\"merges two different data dics of the same year\n",
    "    Vorgehen:\n",
    "    dic mergen, wenn key nicht vorhanden neuen hinzufügen\n",
    "    wenn key schon vorhanden, zu dataframe hinzufügen, aber prüfen ob eintrag schon existiert\n",
    "    \"\"\"\n",
    "    data_new=defaultdict(dict)\n",
    "\n",
    "    for channel in channels:\n",
    "        c=str(channel)\n",
    "\n",
    "        # if channel is not avaible in one dic but in the other take the data from the dic\n",
    "        if c not in data_A.keys() and c in data_B.keys():\n",
    "            data_new[c]=data_B[c]\n",
    "\n",
    "        elif c not in data_B.keys() and c in data_A.keys():\n",
    "            data_new[c]=data_A[c]\n",
    "\n",
    "        # if channel is in both look at months\n",
    "        elif c in data_B.keys() and c in data_A.keys():\n",
    "            for month in months:\n",
    "                m=str(month)\n",
    "\n",
    "                # if month is not avaible in one dic but in the other take the data from the dic\n",
    "                if m not in data_A[c].keys() and m in data_B[c].keys():\n",
    "                    data_new[c][m]=data_B[c][m]\n",
    "\n",
    "                elif m not in data_B[c].keys() and m in data_A[c].keys():\n",
    "                    data_new[c][m]=data_A[c][m]\n",
    "                \n",
    "                #if month is in both check if dataframes are equal\n",
    "                elif m in data_B[c].keys() and m in data_A[c].keys():\n",
    "                    \n",
    "                    # if both have same index, theay are equal. So I can take one of them\n",
    "                    # kann ich eigentlich weglassen und immer concaten und drop duplicate machen\n",
    "                    # so ist aber schneller\n",
    "                    if len(data_B[c][m].index) == len(data_A[c][m].index):\n",
    "                        if data_B[c][m].index == data_A[c][m].index: #if len are not eyqual this gives an error\n",
    "                            data_new[c][m] = data_A[c][m]\n",
    "                        \n",
    "                        else: #copy of other else\n",
    "                            #concat dataframes, drop duplicetes and sort\n",
    "                            data_new[c][m] = pd.concat([data_A[c][m],data_B[c][m]],axis=0).drop_duplicates()\n",
    "                            data_new[c][m].sort_index(axis=0,inplace=True)\n",
    "                            print(f\"concat for channel {c}, month {m} (other else)\")\n",
    "\n",
    "                    # else they dont have the same index, one contains more / additional data\n",
    "                    else:\n",
    "                        #concat dataframes, drop duplicetes and sort\n",
    "                        data_new[c][m] = pd.concat([data_A[c][m],data_B[c][m]],axis=0).drop_duplicates()\n",
    "                        data_new[c][m].sort_index(axis=0,inplace=True)\n",
    "                        print(f\"concat for channel {c}, month {m}\")\n",
    "\n",
    "    return data_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n",
      "concat for channel 1, month 5\n",
      "concat for channel 2, month 5\n",
      "concat for channel 3, month 5\n",
      "concat for channel 4, month 5\n",
      "concat for channel 1, month 6\n",
      "concat for channel 2, month 6\n",
      "concat for channel 3, month 6\n",
      "concat for channel 4, month 6\n",
      "concat for channel 5, month 6\n",
      "concat for channel 6, month 6\n",
      "concat for channel 7, month 6\n",
      "concat for channel 8, month 6\n",
      "concat for channel 1, month 6\n",
      "concat for channel 2, month 6\n",
      "concat for channel 3, month 6\n",
      "concat for channel 4, month 6\n",
      "concat for channel 5, month 6\n",
      "concat for channel 6, month 6\n",
      "concat for channel 7, month 6\n",
      "concat for channel 8, month 6\n",
      "concat for channel 1, month 6\n",
      "concat for channel 1, month 7\n",
      "concat for channel 2, month 6\n",
      "concat for channel 2, month 7\n",
      "concat for channel 3, month 6\n",
      "concat for channel 3, month 7\n",
      "concat for channel 4, month 6\n",
      "concat for channel 4, month 7\n",
      "concat for channel 5, month 6\n",
      "concat for channel 5, month 7\n",
      "concat for channel 6, month 6\n",
      "concat for channel 6, month 7\n",
      "concat for channel 7, month 6\n",
      "concat for channel 7, month 7\n",
      "concat for channel 8, month 6\n",
      "concat for channel 8, month 7\n",
      "concat for channel 1, month 6\n",
      "concat for channel 1, month 7\n",
      "concat for channel 2, month 6\n",
      "concat for channel 2, month 7\n",
      "concat for channel 3, month 6\n",
      "concat for channel 3, month 7\n",
      "concat for channel 4, month 6\n",
      "concat for channel 4, month 7\n",
      "concat for channel 5, month 6\n",
      "concat for channel 5, month 7\n",
      "concat for channel 6, month 6\n",
      "concat for channel 6, month 7\n",
      "concat for channel 7, month 6\n",
      "concat for channel 7, month 7\n",
      "concat for channel 8, month 6\n",
      "concat for channel 8, month 7\n",
      "concat for channel 1, month 7\n",
      "concat for channel 2, month 7\n",
      "concat for channel 3, month 7\n",
      "concat for channel 4, month 7\n",
      "concat for channel 5, month 7\n",
      "concat for channel 6, month 7\n",
      "concat for channel 7, month 7\n",
      "concat for channel 8, month 7\n",
      "concat for channel 1, month 8\n",
      "concat for channel 2, month 8\n",
      "concat for channel 3, month 8\n",
      "concat for channel 4, month 8\n",
      "concat for channel 5, month 8\n",
      "concat for channel 6, month 8\n",
      "concat for channel 7, month 8\n",
      "concat for channel 8, month 8\n",
      "concat for channel 1, month 8\n",
      "concat for channel 2, month 8\n",
      "concat for channel 3, month 8\n",
      "concat for channel 4, month 8\n",
      "concat for channel 5, month 8\n",
      "concat for channel 6, month 8\n",
      "concat for channel 7, month 8\n",
      "concat for channel 8, month 8\n",
      "2020\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "#Wall time: 40.9 s\n",
    "# Merge different dics, which were exported seperate from Charon4\n",
    "#print(\"2021\")\n",
    "#data_2021 = merge_dics_data(data_2021_parts[0], data_2021_parts[1])\n",
    "#print(\"2020\")\n",
    "#data_2020 = merge_dics_data(data_2020_parts[0], data_2020_parts[1])\n",
    "#print(\"2019\")\n",
    "#data_2019 = merge_dics_data(data_2019_parts[0], data_2019_parts[1])\n",
    "\n",
    "def merge_parts(data_20xx_parts):\n",
    "    \"\"\"merges all the different parts\"\"\"\n",
    "\n",
    "    data_20xx = data_20xx_parts[0] #final dataframe\n",
    "    for i in range(1,len(data_20xx_parts)):\n",
    "        data_20xx = merge_dics_data(data_20xx, data_20xx_parts[i])\n",
    "\n",
    "    return data_20xx\n",
    "\n",
    "\n",
    "print(\"2021\")\n",
    "data_2021 = merge_parts(data_2021_parts)\n",
    "print(\"2020\")\n",
    "data_2020 = merge_parts(data_2020_parts)\n",
    "print(\"2019\")\n",
    "data_2019 = merge_parts(data_2019_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with nan, why? Should drop them before saving my_database\n",
    "# maybe due to stopping an running measurement? or my concat?\n",
    "# did not investigate further yet\n",
    "if controller==3188:\n",
    "    data_2021[\"1\"][\"4\"] = data_2021[\"1\"][\"4\"].drop(pd.to_datetime('2021-04-26 08:55:50'),axis=0)\n",
    "    data_2021[\"1\"][\"4\"] = data_2021[\"1\"][\"4\"].drop(pd.to_datetime('2021-04-28 08:28:17'),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Daniel data and merge it\n",
    "Mario gave Geophysica these data. They have been uploaded to moodle. They cover a range, where I did find any data in the Charon4 database. Maybe they have been exported with a different pc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#Wall time: 41.3 s\n",
    "if controller==3188:\n",
    "    # Import data from daniel\n",
    "    def import_data_Daniel(path):\n",
    "        \"\"\"import for data from sciebo Eduardschacht repository\n",
    "        copied from compare script\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path,delimiter = ',',index_col=0, header=7 )\n",
    "        df = df.drop(df.columns[[0,1]], axis=1)\n",
    "        df.index=pd.to_datetime(df.index, dayfirst = True).tz_localize(None)\n",
    "        df.index=df.index.astype(str) # same column format as other dataframe\n",
    "        df=df.transpose() # for having the same format as the other dataframe\n",
    "        df.index=df.index.astype(float) #make depth to int\n",
    "        df.index=df.index.astype(int) #make depth to int\n",
    "        df.index.names = ['Depth [m]']\n",
    "        df.columns.names=[\"Length [m]\"] # was wollte ich hier machen?\n",
    "        return df\n",
    "\n",
    "    def put_files_in_dataframe(list_of_paths,import_function):\n",
    "        \"\"\"\n",
    "        merges files in one dataframe using an import_function (user-defined-functiom) in a list_of paths (list)\n",
    "        copied from compare script --> not updated in comare script!!!\n",
    "        \"\"\"\n",
    "        # Import Data\n",
    "        dic={}\n",
    "        for path in list_of_paths:\n",
    "            dic[path]=import_function(path)\n",
    "\n",
    "        # Merge files into one dataframe\n",
    "        dic_concat=pd.concat(dic.values(),axis=1)\n",
    "        dic_concat.columns = pd.to_datetime(dic_concat.columns)\n",
    "        dic_concat = dic_concat.sort_index(axis=1)\n",
    "\n",
    "        return dic_concat\n",
    "\n",
    "    # Import files\n",
    "    paths_to_working_dir=r\"..\\Alsdorf\\Daten\"\n",
    "    daniel_ch1_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch1\\*\")\n",
    "    daniel_ch2_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch2\\*\")\n",
    "    daniel_ch3_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch3\\*\")\n",
    "    daniel_ch4_paths=get_abspath(paths_to_working_dir + r\"\\Betrieb2019\\Ch4\\*\")\n",
    "    # Merge them in one dataframe\n",
    "    daniel_ch1=put_files_in_dataframe(daniel_ch1_paths,import_data_Daniel)\n",
    "    daniel_ch2=put_files_in_dataframe(daniel_ch2_paths,import_data_Daniel)\n",
    "    daniel_ch3=put_files_in_dataframe(daniel_ch3_paths,import_data_Daniel)\n",
    "    daniel_ch4=put_files_in_dataframe(daniel_ch4_paths,import_data_Daniel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Add daniel data to data_2019\n",
    "if controller==3188:\n",
    "    def add_daniel_data_to_2019(daniel_chx,channel_of_daniel):\n",
    "        \"\"\"\n",
    "        beide Datensätze sind gleich in dem Bereich wo sie sich überschneiden, siehe compare script\n",
    "\n",
    "        daniel data will overwrite data in data_2019    \n",
    "        \"\"\"\n",
    "        # Change the format of daniel data, so that it fits to data_2019\n",
    "        daniel_chx.index.names=[\"\"]\n",
    "        daniel_chx=daniel_chx.transpose() #----geändert vorher ch2 - war bug!\n",
    "        daniel_chx.index.names=[\"Date\"]\n",
    "        #daniel_chx.index=pd.to_datetime(daniel_chx.index) #now in put_files_in_dataframe()\n",
    "\n",
    "        # Group data by month and create seperate dataframes\n",
    "        # groupby your key / level and freq\n",
    "        g = daniel_chx.groupby(pd.Grouper(level='Date', freq='M'))\n",
    "        # groups to a list of dataframes with list comprehension\n",
    "        dfs = [group for _,group in g]\n",
    "\n",
    "        # Add the seperate dataframes from group in a dictionary, so they have the same format as data_2019\n",
    "        daniel_dic=defaultdict(dict)\n",
    "        count_month=3 # first month of daniel data, which will used as an overwrite\n",
    "        for dataframe in dfs[1:]: #skip first one, so count_month fits--------------geändert - war bug!\n",
    "            daniel_dic[channel_of_daniel][str(count_month)]=dataframe\n",
    "            count_month+=1\n",
    "            \n",
    "        # Add data to 2019 dataframe\n",
    "        for month in daniel_dic[channel_of_daniel].keys():\n",
    "            data_2019[channel_of_daniel][month]=daniel_dic[channel_of_daniel][month]\n",
    "        return daniel_dic\n",
    "    #Add the data\n",
    "    print(\"1\")\n",
    "    daniel_dic_1=add_daniel_data_to_2019(daniel_ch1,\"1\")\n",
    "    print(\"2\")\n",
    "    daniel_dic_2=add_daniel_data_to_2019(daniel_ch2,\"2\")\n",
    "    print(\"3\")\n",
    "    daniel_dic_3=add_daniel_data_to_2019(daniel_ch3,\"3\")\n",
    "    print(\"4\")\n",
    "    daniel_dic_4=add_daniel_data_to_2019(daniel_ch4,\"4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Wall time: 12min 25s only csv\n",
    "# Wall time: 100 s only pickle\n",
    "def data_save_helper(data_20xx,channel):\n",
    "    \"\"\"merges different months to one file\"\"\"\n",
    "    # join all month, so that the data is saved\n",
    "    first_month=list(data_20xx[channel].keys())[0]\n",
    "    one_file=data_20xx[channel][first_month]\n",
    "    for month in list(data_20xx[channel].keys())[1:]: #skip first month, defined above\n",
    "        one_file = pd.concat([one_file, data_20xx[channel][month]], axis=0)\n",
    "    # sort all dates, in 2019 is month 9 before 10. Did not yet check how this happened, \n",
    "    # but this is solved with this line anyway\n",
    "    one_file = one_file.sort_index()\n",
    "    #one_file = round(one_file,2) # not needed\n",
    "    return one_file\n",
    "\n",
    "def save_data_csv(data_20xx,save_to_path):\n",
    "    \"\"\"\"\"\"\n",
    "    for channel in data_20xx.keys():\n",
    "        one_file = data_save_helper(data_20xx,channel)\n",
    "        \n",
    "        # save data\n",
    "        year=one_file.index[10].strftime(\"%Y\") #randomly chose 10; very bad solution\n",
    "        filename=f\"\\\\temp_ch{channel}_{year}.csv\"\n",
    "        one_file.columns.names=[\"Length [m]\"]\n",
    "        one_file.to_csv(save_to_path + filename)\n",
    "\n",
    "def write_pickle_custom(save_to:str,data_20xx):\n",
    "    \"\"\"I adaptded this function only for this notebook\"\"\"\n",
    "    #Function to write pickle Files\n",
    "    for channel in data_20xx.keys():\n",
    "        one_file = data_save_helper(data_20xx,channel)\n",
    "\n",
    "        # save data\n",
    "        year=one_file.index[10].strftime(\"%Y\") #randomly chose 10; very bad solution\n",
    "        filename=f\"\\\\temp_ch{channel}_{year}\"\n",
    "        with open(save_to + filename, 'wb') as handle:\n",
    "            pickle.dump(one_file, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Which dataformat is best? Thats nice short overview:\n",
    "# https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d\n",
    "# Save data\n",
    "path_to_my_database_Alsdorf = r\"..\\Alsdorf\\Daten\\my_database\"\n",
    "path_to_my_database_EONERC = r\"C:\\Users\\Mathis\\ownCloud\\my_EONERC_data\\data_from_charon4\"\n",
    "\n",
    "if controller==3188:\n",
    "    path_to_my_database = path_to_my_database_Alsdorf\n",
    "\n",
    "if controller==3195:\n",
    "    path_to_my_database = path_to_my_database_EONERC\n",
    "\n",
    "save_to = path_to_my_database + \"\\csv\"\n",
    "if data_save_csv:\n",
    "    save_data_csv(data_2021,save_to)\n",
    "    save_data_csv(data_2020,save_to)\n",
    "    save_data_csv(data_2019,save_to)\n",
    "if data_save_pickle:\n",
    "    save_to_pickle = path_to_my_database + \"\\pickle\"\n",
    "    write_pickle_custom(save_to_pickle, data_2021)\n",
    "    write_pickle_custom(save_to_pickle, data_2020)\n",
    "    write_pickle_custom(save_to_pickle, data_2019)\n",
    "\n",
    "    # save all in one file\n",
    "    # nicht gerade optimal gelöst mit dem neu einladen\n",
    "    if controller==3188:\n",
    "        def save_all(): #variables will not be saved outside the function\n",
    "            data_2021=import_my_database_pickle(2021, path_to_my_database + \"\\pickle\")\n",
    "            data_2020=import_my_database_pickle(2020, path_to_my_database + \"\\pickle\")\n",
    "            data_2019=import_my_database_pickle(2019, path_to_my_database + \"\\pickle\")\n",
    "            write_pickle(path_to_my_database + \"\\..\\my_database_additional_local\\data_all\",merge_data_year([data_2019, data_2020, data_2021]))\n",
    "        save_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data - example\n",
    "# temp_ch1_2021.csv temp_ch2_2021.csv temp_ch3_2021.csv temp_ch4_2021.csv\n",
    "# temp_ch1_2020.csv temp_ch2_2020.csv temp_ch3_2020.csv temp_ch4_2020.csv\n",
    "# temp_ch1_2019.csv temp_ch2_2019.csv temp_ch3_2019.csv temp_ch4_2019.csv\n",
    "#filename=f\"\\\\temp_ch2_2020.csv\" \n",
    "#test=pd.read_csv(save_to + filename, index_col=0)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-09-17 10:26:41')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_month=list(data_2021[\"1\"].keys())[-1]\n",
    "last_measurement_2021 = data_2021[\"1\"][last_month].index[-1]\n",
    "last_measurement_2021 #last measurement of channel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
