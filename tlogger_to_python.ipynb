{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tlogger data\n",
    "here I will import the raw tlogger data and save them in an easy to acces format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Version information of the imported packages\n",
      "pandas version: 1.2.3\n",
      "pickle version: 4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from my_func_mvw.functions import find_nearest_date, read_pickle, write_pickle, temp_watertank_func, random_date\n",
    "\n",
    "plot_save=True #True False\n",
    "\n",
    "print(\"Some Version information of the imported packages\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"pickle version: {pickle.format_version}\")\n",
    "# Some Version information of the imported packages\n",
    "# pandas version: 1.2.3\n",
    "# pickle version: 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_t_logger(path_to_data_Tlogger, filename_Tlogger, data_format=\"pc_local\",channel_name=\"none\"):\n",
    "    \"\"\"\"\"\"\n",
    "    if data_format==\"CF_Card\": #auf Speicherkarte speichern: \"stand alone modus\"\n",
    "        df_Tlogger = pd.read_csv(path_to_data_Tlogger + \"\\\\\" + filename_Tlogger, sep=\";\", usecols=[1,2,3,4], names=[\"Date\",\"Time\",\"Channel1-Watertank_PT100\",\"Channel2-Air\"])\n",
    "        DateTime=df_Tlogger[\"Date\"] + \" \" + df_Tlogger[\"Time\"]\n",
    "        df_Tlogger.index =pd.to_datetime(DateTime, format=(\"%d.%m.%y  %H:%M:%S\"))\n",
    "        df_Tlogger.index.names = ['Date']\n",
    "        df_Tlogger = df_Tlogger.drop(df_Tlogger.columns[0:2],axis=1) # drop the two columns which are now used for index\n",
    "\n",
    "    if data_format == \"pc_local\": #direkt auf pc speichern\n",
    "        #channel 2 wird im moment hier nicht eingeladen, da das datum sich teilweise um wenige sekunden unterscheided\n",
    "        #müsste beide indices noch anpassen so das sie zusammenpassen\n",
    "        def import_helper(filename,channel_name):\n",
    "            df_Tlogger = pd.read_csv(\n",
    "                filename, sep=\"\\s+|\\t| \", usecols=[0,1,2],\n",
    "                names=[\"Date\",\"Time\",channel_name], engine=\"python\")\n",
    "            DateTime=df_Tlogger[\"Date\"] + \" \" + df_Tlogger[\"Time\"]\n",
    "            df_Tlogger.index =pd.to_datetime(DateTime, format=(\"%d.%m.%Y %H:%M:%S\"))\n",
    "            df_Tlogger.index.names = ['Date']\n",
    "            df_Tlogger = df_Tlogger.drop(df_Tlogger.columns[0:2],axis=1) # drop the two columns which are now used for index\n",
    "            return df_Tlogger\n",
    "\n",
    "        df_Tlogger = import_helper(path_to_data_Tlogger + \"\\\\\" + filename_Tlogger,channel_name)\n",
    "        \n",
    "    return df_Tlogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1-Watertank_PT100</th>\n",
       "      <th>Channel2-Air</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:12:40</th>\n",
       "      <td>24.948476</td>\n",
       "      <td>25.819904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:13:40</th>\n",
       "      <td>25.026853</td>\n",
       "      <td>25.443956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:14:40</th>\n",
       "      <td>24.888809</td>\n",
       "      <td>25.165027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:15:40</th>\n",
       "      <td>25.011371</td>\n",
       "      <td>24.926351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-07 12:16:40</th>\n",
       "      <td>25.014596</td>\n",
       "      <td>24.753213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:34:54</th>\n",
       "      <td>22.817576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:37:54</th>\n",
       "      <td>22.811149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:40:54</th>\n",
       "      <td>22.811149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:43:54</th>\n",
       "      <td>22.830430</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-02 11:46:54</th>\n",
       "      <td>22.772589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21766 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Channel1-Watertank_PT100  Channel2-Air\n",
       "Date                                                       \n",
       "2021-06-07 12:12:40                 24.948476     25.819904\n",
       "2021-06-07 12:13:40                 25.026853     25.443956\n",
       "2021-06-07 12:14:40                 24.888809     25.165027\n",
       "2021-06-07 12:15:40                 25.011371     24.926351\n",
       "2021-06-07 12:16:40                 25.014596     24.753213\n",
       "...                                       ...           ...\n",
       "2021-08-02 11:34:54                 22.817576           NaN\n",
       "2021-08-02 11:37:54                 22.811149           NaN\n",
       "2021-08-02 11:40:54                 22.811149           NaN\n",
       "2021-08-02 11:43:54                 22.830430           NaN\n",
       "2021-08-02 11:46:54                 22.772589           NaN\n",
       "\n",
       "[21766 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data T-logger\n",
    "path_to_data_Tlogger = r\"..\\Alsdorf\\Daten\\T-logger\"\n",
    "# PT100 sensors\n",
    "# channel1 in watertank PT100\n",
    "# channel2 air PT1000\n",
    "filename_Tlogger_0 = \"20210608\\_adc_START_07.06.2021-12_11_40_daten.txt\" \n",
    "filename_Tlogger_1 = \"20210702\\_adc_START_08.06.2021-13_21_47_daten.txt\"\n",
    "filename_Tlogger_2 = \"20210708\\_adc_START_02.07.2021-12_36_14_daten.txt\"\n",
    "filenames_CFCard = [filename_Tlogger_0,filename_Tlogger_1,filename_Tlogger_2]\n",
    "# new dataformat\n",
    "filename_Tlogger_3 = \"20210722\\kanal_1.txt\"\n",
    "filename_Tlogger_4 = \"20210727\\kanal_1.txt\"\n",
    "filename_Tlogger_5 = \"20210802\\kanal_1.txt\"\n",
    "filenames_pc = [filename_Tlogger_3,filename_Tlogger_4,filename_Tlogger_5]\n",
    "\n",
    "df_Tlogger_PT100_part={}\n",
    "for filename in filenames_CFCard:\n",
    "    df_Tlogger_PT100_part[filename] = import_t_logger(path_to_data_Tlogger, filename, data_format=\"CF_Card\")\n",
    "for filename in filenames_pc:\n",
    "    df_Tlogger_PT100_part[filename] = import_t_logger(path_to_data_Tlogger, filename, channel_name = \"Channel1-Watertank_PT100\")\n",
    "\n",
    "# concat all dataframes\n",
    "df_Tlogger_PT100 = df_Tlogger_PT100_part[filenames_CFCard[0]]\n",
    "for dataframe in list(df_Tlogger_PT100_part.keys())[1:]:\n",
    "    df_Tlogger_PT100 = pd.concat([df_Tlogger_PT100,df_Tlogger_PT100_part[dataframe]])\n",
    "df_Tlogger_PT100.sort_index(inplace=True)\n",
    "\n",
    "df_Tlogger_PT100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PT1000 sensors\n",
    "# two new sensors, both are now in watertank, I create new dataframe\n",
    "######## Add new, when downloading new data from t_logger###################\n",
    "filename_Tlogger_5_1 = \"20210809\\kanal_1.txt\"\n",
    "filename_Tlogger_5_2 = \"20210809\\kanal_2.txt\"\n",
    "filenames_pc_1 = [filename_Tlogger_5_1]\n",
    "filenames_pc_2 = [filename_Tlogger_5_2]\n",
    "##########################################################################\n",
    "\n",
    "#import data\n",
    "df_Tlogger_PT1000_part={}\n",
    "for filename in filenames_pc_1:\n",
    "    df_Tlogger_PT1000_part[filename] = import_t_logger(path_to_data_Tlogger, filename, channel_name = \"Channel1_PT1000\")\n",
    "for filename in filenames_pc_2:\n",
    "    df_Tlogger_PT1000_part[filename] = import_t_logger(path_to_data_Tlogger, filename, channel_name = \"Channel2_PT1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only relevant for data_format = \"pc_local\"\n",
    "# these chanels can have different indexes, with a very small difference\n",
    "# I will make them more simialr to avoid a lot of nan numbers in the dataframe\n",
    "new_index = []\n",
    "counter = 0\n",
    "for i in range(len(df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'].index)): # go through all dates\n",
    "    date = df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'].index[i]\n",
    "    # find date_name in other channel, which is closest to the date of channel 2\n",
    "    date_name, date_iloc = find_nearest_date(date, df_Tlogger_PT1000_part['20210809\\\\kanal_1.txt'].index)\n",
    "    timediff = pd.to_datetime(date_name) - date\n",
    "\n",
    "    if timediff < timedelta(seconds = 0): # make timediff positive, if its negative\n",
    "        timediff = timediff * -1\n",
    "\n",
    "    if timediff < timedelta(seconds = 15):\n",
    "        # temp_val = df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'].values[i]\n",
    "        # dataframe = pd.DataFrame(columns=[\"Channel2_PT1000\"], index=[pd.to_datetime(date_name)])\n",
    "        # dataframe[\"Channel2_PT1000\"] = temp_val\n",
    "        # dataframe.index.names = [\"Date\"]\n",
    "        # df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'][\"Channel2_PT1000\"][i] = dataframe\n",
    "\n",
    "        new_index.append(pd.to_datetime(date_name))\n",
    "    else:\n",
    "        new_index.append(pd.to_datetime(date))\n",
    "        counter += 1\n",
    "\n",
    "df_Tlogger_PT1000_part['20210809\\\\kanal_2.txt'].index = new_index\n",
    "\n",
    "if counter > 0:\n",
    "    print(f\"timediff larger than 15s {counter} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1_PT1000</th>\n",
       "      <th>Channel2_PT1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:18:42</th>\n",
       "      <td>20.453140</td>\n",
       "      <td>20.464840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:21:42</th>\n",
       "      <td>20.446965</td>\n",
       "      <td>20.455567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:24:42</th>\n",
       "      <td>20.448508</td>\n",
       "      <td>20.440113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:27:42</th>\n",
       "      <td>20.426124</td>\n",
       "      <td>20.448613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-04 11:30:42</th>\n",
       "      <td>20.433071</td>\n",
       "      <td>20.457113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-09 10:28:14</th>\n",
       "      <td>20.637639</td>\n",
       "      <td>20.492660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-09 10:31:14</th>\n",
       "      <td>20.624515</td>\n",
       "      <td>20.489569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-09 10:34:14</th>\n",
       "      <td>20.637639</td>\n",
       "      <td>20.486478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-09 10:37:13</th>\n",
       "      <td>20.628375</td>\n",
       "      <td>20.488023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-09 10:40:13</th>\n",
       "      <td>20.620655</td>\n",
       "      <td>20.488023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2388 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Channel1_PT1000  Channel2_PT1000\n",
       "2021-08-04 11:18:42        20.453140        20.464840\n",
       "2021-08-04 11:21:42        20.446965        20.455567\n",
       "2021-08-04 11:24:42        20.448508        20.440113\n",
       "2021-08-04 11:27:42        20.426124        20.448613\n",
       "2021-08-04 11:30:42        20.433071        20.457113\n",
       "...                              ...              ...\n",
       "2021-08-09 10:28:14        20.637639        20.492660\n",
       "2021-08-09 10:31:14        20.624515        20.489569\n",
       "2021-08-09 10:34:14        20.637639        20.486478\n",
       "2021-08-09 10:37:13        20.628375        20.488023\n",
       "2021-08-09 10:40:13        20.620655        20.488023\n",
       "\n",
       "[2388 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all data\n",
    "df_Tlogger_PT1000 = df_Tlogger_PT1000_part[filename_Tlogger_5_1]\n",
    "for dataframe in list(df_Tlogger_PT1000_part.keys())[1:]:\n",
    "    df_Tlogger_PT1000 = pd.concat([df_Tlogger_PT1000,df_Tlogger_PT1000_part[dataframe]],axis=1)\n",
    "df_Tlogger_PT1000.sort_index(inplace=True)\n",
    "df_Tlogger_PT1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from solexperts, which can be used for my calibration\n",
    "# import solexperts t_logger\n",
    "path_to_my_database = r\"..\\Alsdorf\\Daten\\my_database\"\n",
    "path_to_solexperts = path_to_my_database + \"\\Solexperts_EGRT\"\n",
    "filename = \"\\wagoTemperatur_korigiert.txt\"\n",
    "tlogger_sol = pd.read_csv(path_to_solexperts + filename, delimiter=\"\\t\", index_col=0)\n",
    "tlogger_sol.columns.names = [\"Dates\"]\n",
    "tlogger_sol.columns= pd.to_datetime(tlogger_sol.columns)\n",
    "tlogger_sol.index=[\"Watertank\"]\n",
    "tlogger_sol.index.names = [\"\"]\n",
    "tlogger_sol = tlogger_sol.T\n",
    "tlogger_sol.columns = [\"Channel1-Watertank_PT100\"] # name is does not fit to this data, just for concat woth my other dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übergangslösung bis PT1000 mehr Daten hat\n",
    "# correct old data\n",
    "df_Tlogger_PT100[\"Channel1-Watertank_PT100\"] = df_Tlogger_PT100[\"Channel1-Watertank_PT100\"] - 1.5\n",
    "\n",
    "# add PT1000 data\n",
    "df_Tlogger_übergang = pd.DataFrame(columns=[\"Channel1-Watertank_PT100\",\"Channel2-Air\"]) #,\"Channel1-PT100_rolling_mean\"\n",
    "df_Tlogger_übergang[\"Channel1-Watertank_PT100\"] = df_Tlogger_PT1000[\"Channel1_PT1000\"]\n",
    "#df_Tlogger_übergang[\"Channel1-PT100_rolling_mean\"] = df_Tlogger_PT1000[\"Channel1-PT1000_rolling_mean\"]\n",
    "df_Tlogger_PT100 = pd.concat([df_Tlogger_PT100, df_Tlogger_übergang]).sort_index()\n",
    "\n",
    "# add also some data from solexperts, which can be used for my calibration\n",
    "    # --> outside heating and cooling phase\n",
    "    # drop the data of our dataframe before concat, so nothing is doubled\n",
    "df_Tlogger_PT100 = pd.concat([df_Tlogger_PT100.drop(df_Tlogger_PT100[16160:16160+410].index), tlogger_sol[6000:]]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_PT100\n",
      "5914 dates with nan have been added\n",
      "df_PT1000\n",
      "0 dates with nan have been added\n"
     ]
    }
   ],
   "source": [
    "# Add nan values in data gaps\n",
    "\n",
    "# find data gaps\n",
    "def add_nan_val_in_datagaps(df_Tlogger):\n",
    "    \"\"\"\"\"\"\n",
    "    diff = df_Tlogger.index[1:] - df_Tlogger.index[:-1]\n",
    "\n",
    "    index_datagaps=[]\n",
    "    for i in range(len(diff)):\n",
    "        bo = diff[i] > timedelta(minutes=10)\n",
    "        if bo == True:\n",
    "            index_datagaps.append(i)\n",
    "            #print(i)#;print(diff[i]) #i gibt einem die position des Datums ab welchem danach die Lücke ist\n",
    "\n",
    "    # add values in data gaps\n",
    "    n_appended_values = 0 # count the number of values I add to the dataframe\n",
    "    for index in index_datagaps:\n",
    "        index_corrected = index + n_appended_values # the dataframe gets new values in other date gaps before this one\n",
    "        # number of dates with nan, that will be added behin the index position\n",
    "        # round down and minus 1 to be sure I dont add a nan behind an existing date\n",
    "        n_nan_dates = math.floor(diff[index] / timedelta(minutes=3)) - 1\n",
    "        n_appended_values += n_nan_dates\n",
    "        list_nan = [np.nan] * n_nan_dates # list of nan\n",
    "\n",
    "        # add nan's in data gaps, by creating new datapoints with a timedifference of 3 min.\n",
    "        # create dataframe which contains the dates and nan values\n",
    "        new_val = pd.DataFrame({df_Tlogger.columns[0]: list_nan, df_Tlogger.columns[1]: list_nan}) #, df_Tlogger.columns[2]: list_nan}\n",
    "        new_val.index = [df_Tlogger.index[index_corrected] + timedelta(minutes=x*3) for x in range(1,n_nan_dates+1)]\n",
    "\n",
    "        # add nan values to Tlogger dataframe and sort it\n",
    "        df_Tlogger = df_Tlogger.append(new_val).sort_index()\n",
    "    print(f\"{n_appended_values} dates with nan have been added\")\n",
    "\n",
    "\n",
    "    # Check if everything worked correct\n",
    "    # find data gaps\n",
    "    diff = df_Tlogger.index[1:] - df_Tlogger.index[:-1]\n",
    "    for i in range(len(diff)):\n",
    "        bo = diff[i] > timedelta(minutes=10)\n",
    "        if bo == True:\n",
    "            print(\"There are still some indexes missing:\")\n",
    "            print(i);print(diff[i]);print() #i gibt einem die position des Datums ab welchem danach die Lücke ist\n",
    "    \n",
    "    return df_Tlogger\n",
    "\n",
    "print(\"df_PT100\")\n",
    "df_Tlogger_PT100 = add_nan_val_in_datagaps(df_Tlogger_PT100)\n",
    "print(\"df_PT1000\")\n",
    "df_Tlogger_PT1000 = add_nan_val_in_datagaps(df_Tlogger_PT1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Moving Avearage\n",
    "#https://towardsdatascience.com/moving-averages-in-python-16170e20f6c nice explanation\n",
    "df_Tlogger_PT100[\"Channel1-PT100_rolling_mean\"]=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].rolling(5,min_periods=3, center = True).mean() # 15 min window time\n",
    "\n",
    "df_Tlogger_PT1000[\"Channel1-PT1000_rolling_mean\"]=df_Tlogger_PT1000[\"Channel1_PT1000\"].rolling(5,min_periods=3, center = True).mean() # 15 min window time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round data to significant position\n",
    "df_Tlogger_PT100 = round(df_Tlogger_PT100,1)\n",
    "df_Tlogger_PT1000 = round(df_Tlogger_PT1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tlogger data to my_database\n",
    "path_to_my_database = r\"..\\Alsdorf\\Daten\\my_database\"\n",
    "\n",
    "df_Tlogger_PT100.to_csv(path_to_my_database + \"/t_logger_watertank/Tlogger_PT100_outdated.csv\")\n",
    "write_pickle(path_to_my_database + \"/t_logger_watertank/Tlogger_PT100_outdated\",df_Tlogger_PT100)\n",
    "\n",
    "df_Tlogger_PT1000.to_csv(path_to_my_database + \"/t_logger_watertank/Tlogger_PT1000.csv\")\n",
    "write_pickle(path_to_my_database + \"/t_logger_watertank/Tlogger_PT1000\",df_Tlogger_PT1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Timestamp' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-333669f1daa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_Tlogger_PT100\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Channel1-Watertank_PT100\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mx_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_Tlogger_PT100\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Channel1-Watertank_PT100\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mT_avearage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_watertank_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_date_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_Tlogger_PT100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m# I could also plot it like this, but this does not show, that my function works\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#plt.plot(df_Tlogger[\"Channel1-rolling_mean\"].index,df_Tlogger[\"Channel1-rolling_mean\"].values,label=\"Watertank rolling mean\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mathis\\Desktop\\Masterarbeit\\Scripts\\my_func_mvw\\functions.py\u001b[0m in \u001b[0;36mtemp_watertank_func\u001b[1;34m(x, df_Tlogger, channel_name)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;31m# check timediff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mtimediff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimediff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Timestamp' and 'list'"
     ]
    }
   ],
   "source": [
    "# Plot T-Logger data\n",
    "number_test_watertank_function = 300 #Input\n",
    "plot_function_check = True #Input\n",
    "plot_air = True #Input\n",
    "\n",
    "watertank_T_range_min = df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].dropna().index.min()\n",
    "watertank_T_range_max = df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].dropna().index.max()\n",
    "\n",
    "# generate random dates in range of T-Logger dates\n",
    "random_date_list=[]\n",
    "for number in range(number_test_watertank_function):\n",
    "    r_date = random_date(watertank_T_range_min,watertank_T_range_max)\n",
    "    random_date_list.append(r_date)\n",
    "\n",
    "# Plot function and data, to see if it is good\n",
    "y=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].values\n",
    "x_dates=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"].index\n",
    "T_avearage=temp_watertank_func(random_date_list, df_Tlogger_PT100)\n",
    "# I could also plot it like this, but this does not show, that my function works\n",
    "#plt.plot(df_Tlogger[\"Channel1-rolling_mean\"].index,df_Tlogger[\"Channel1-rolling_mean\"].values,label=\"Watertank rolling mean\")\n",
    "plt.figure(figsize=(16,5))\n",
    "if plot_air:\n",
    "    plt.plot(df_Tlogger_PT100[\"Channel2-Air\"].index,df_Tlogger_PT100[\"Channel2-Air\"].values,label=\"Air-Temp\",color=\"black\",zorder=8)\n",
    "plt.plot(x_dates,y,label=\"PT100 Watertank-Temp\",color=\"blue\",zorder=9)\n",
    "if plot_function_check:\n",
    "    plt.scatter(random_date_list,T_avearage,label=\"Watertank temperature function\\nat random positions\",color=\"green\",zorder=10)\n",
    "\n",
    "# y=df_Tlogger_PT1000[\"Channel1_PT1000\"].dropna().values\n",
    "# x_dates=df_Tlogger_PT1000[\"Channel1_PT1000\"].dropna().index\n",
    "# plt.plot(x_dates,y,label=\"PT1000-1 Watertank-Temp\",color=\"purple\")\n",
    "\n",
    "# y=df_Tlogger_PT1000[\"Channel2_PT1000\"].dropna().values\n",
    "# x_dates=df_Tlogger_PT1000[\"Channel2_PT1000\"].dropna().index\n",
    "# plt.plot(x_dates,y,label=\"PT1000-2 Watertank-Temp\",color=\"blue\")\n",
    "\n",
    "# y=df_Tlogger[\"Channel1-rolling_mean\"].values\n",
    "# x_dates=df_Tlogger[\"Channel1-rolling_mean\"].index\n",
    "# plt.scatter(x_dates,y,label=\"Watertank-Temp rolling_mean\",color=\"red\",zorder=10)\n",
    "\n",
    "plt.ylabel(\"T [°C]\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.title(\"T-Logger\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Wassertank am 07.06.2021 aufgefüllt, mit kälteren Wasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,1,figsize=(16,10))\n",
    "ymin=19; ymax=29\n",
    "dftlog=df_Tlogger_PT100[\"Channel1-Watertank_PT100\"]\n",
    "dftlog_rolling=df_Tlogger_PT100[\"Channel1-PT100_rolling_mean\"]\n",
    "dftlog_sol=tlogger_sol[\"Channel1-Watertank_PT100\"][100:] #skip first ones because they are very high\n",
    "# are not perfectly chosen, but due to data gaps its looks ok\n",
    "border_temp1=20000\n",
    "border_temp2=26000\n",
    "border_temp3=32500\n",
    "\n",
    "axs[0].set_title(\"Water Tank Temperature\", fontsize=\"13\")\n",
    "axs[0].plot(dftlog[:border_temp1].index,dftlog[:border_temp1].values + 1.5,color=\"grey\", label=\"PT100 original measured\")\n",
    "axs[0].plot(dftlog[:border_temp1].index,dftlog[:border_temp1].values,color=\"red\", label=\"PT100 manually corrected\")\n",
    "\n",
    "axs[0].plot(dftlog[border_temp2:border_temp3].index,dftlog[border_temp2:border_temp3].values +1.5,color=\"grey\")\n",
    "axs[0].plot(dftlog[border_temp2:border_temp3].index,dftlog[border_temp2:border_temp3].values,color=\"red\")\n",
    "\n",
    "axs[0].plot(dftlog_sol.index,dftlog_sol.values, color=\"black\", label=\"Solexperts-Sensor\")\n",
    "\n",
    "axs[0].plot(dftlog[border_temp3:].index,dftlog[border_temp3:].values,color=\"blue\", label=\"PT1000\")\n",
    "\n",
    "\n",
    "axs[1].set_title(\"Data Used For Calibration - Rolling Mean\", fontsize=\"13\")\n",
    "axs[1].plot(dftlog_rolling[:border_temp1].index,dftlog_rolling[:border_temp1].values,color=\"red\", label=\"PT100 manually corrected\")\n",
    "\n",
    "axs[1].plot(dftlog_rolling[border_temp2:border_temp3].index,dftlog_rolling[border_temp2:border_temp3].values,color=\"red\")\n",
    "\n",
    "axs[1].plot(dftlog_rolling[border_temp1:border_temp2].index,dftlog_rolling[border_temp1:border_temp2].values,color=\"black\", label=\"Solexperts-Sensor\")\n",
    "\n",
    "axs[1].plot(dftlog_rolling[border_temp3:].index,dftlog_rolling[border_temp3:].values,color=\"blue\", label=\"PT1000\")\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "    ax.set_ylabel(\"Temperature [°C]\")\n",
    "    #ax.set_xlabel(\"Date\")\n",
    "    legend = ax.legend(fontsize=11, title_fontsize=12, frameon=True, loc=\"upper right\")\n",
    "    legend.get_frame().set_alpha(0.7) #not supported with eps\n",
    "    legend.get_frame().set_facecolor(\"white\")\n",
    "\n",
    "if plot_save:\n",
    "    plt.savefig(r\"..\\Masterthesis_tex\\figs\\chap4\\watertank_temperature.pdf\", format=\"pdf\",bbox_inches=\"tight\")\n",
    "    plt.savefig(\"pictures\\watertank_temperature.png\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
