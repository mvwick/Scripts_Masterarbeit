{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit (conda)"
  },
  "interpreter": {
   "hash": "29bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Merge Files\r\n",
    "This script works with data in `sciebo\\DTS Data\\Alsdorf\\Daten\\DTS_unprocessed\\Daten_Mario_oder_alt`, Alsdorf data channels 1 to 4.\r\n",
    "\r\n",
    "This script converts the data exported with charon4 (export was not done by me), into a better format for processing them with python.\r\n",
    "This script merges multiple smaller files into a bigger one.\r\n",
    "\r\n",
    "the final filename: `temp_chx_year_month_vx.csv`\r\n",
    "* temp: Temperature\r\n",
    "* chx: Channel, x: number\r\n",
    "* year\r\n",
    "* month\r\n",
    "* vx: version of this script\r\n",
    "\r\n",
    "The main folder, which contains all files (about 500000) is `.zip`. It also contains the outputs of this script.\r\n",
    "The data is still in the Charon4 Software and can be exported. It is also in `my_database`.\r\n",
    "Therefore the data (the `.zip` file) is not needed, see also `compare.ipynb`.\r\n",
    "\r\n",
    "## Outdated\r\n",
    "this script is not needed anymore because the data in `Daten_Mario_oder_alt` is the same as in the used database.\r\n",
    "Its just kept here if someone wants to check this again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "\r\n",
    "from my_func_mvw.functions import get_abspath, file_len, number_files\r\n",
    "\r\n",
    "# check of date deactivated because it takes very very long"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "activate_inputs=['temp_ch1_2018_6', 'temp_ch1_2018_7', 'temp_ch1_2018_11', 'temp_ch1_2019_2', 'temp_ch1_2019_3', 'temp_ch1_2019_4', 'temp_ch1_2019_5', 'temp_ch2_2018_6', 'temp_ch2_2018_7', 'temp_ch2_2019_2', 'temp_ch2_2019_3', 'temp_ch2_2019_4', 'temp_ch2_2019_5', 'temp_ch3_2018_6', 'temp_ch3_2019_2', 'temp_ch3_2019_3', 'temp_ch3_2019_4', 'temp_ch3_2019_5', 'temp_ch4_2018_6', 'temp_ch4_2019_2', 'temp_ch4_2019_3', 'temp_ch4_2019_4', 'temp_ch4_2019_5', 'temp_ch5_2018_6', 'temp_ch6_2018_6', 'temp_ch7_2018_11']\r\n",
    "\r\n",
    "############################## Input############################################\r\n",
    "# one or more of activate_inputs\r\n",
    "# using all takes very long and may not use the most up-to-date version of this notebook\r\n",
    "activate=activate_inputs[:]\r\n",
    "#Example: activate_inputs[:]; activate_inputs[5]; activate_inputs[22:25]\r\n",
    "path_wdir=r\"..\\Alsdorf\\Daten\\DTS_unprocessed\\Daten_Mario_oder_alt\" #path to working directory\r\n",
    "#################################################################################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "if type(activate) == str: # it is later assumed activate is a list of names\r\n",
    "    activate=[activate]\r\n",
    "\r\n",
    "# path which will be activated\r\n",
    "# all files in each of these paths will be merged\r\n",
    "paths_for_activate={}\r\n",
    "# channel 1\r\n",
    "paths_for_activate[\"temp_ch1_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\6\"\r\n",
    "paths_for_activate[\"temp_ch1_2018_7\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\7\"\r\n",
    "paths_for_activate[\"temp_ch1_2018_11\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\11\"\r\n",
    "paths_for_activate[\"temp_ch1_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\2\"\r\n",
    "paths_for_activate[\"temp_ch1_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\3\"\r\n",
    "paths_for_activate[\"temp_ch1_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\4\"\r\n",
    "paths_for_activate[\"temp_ch1_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\5\"\r\n",
    "# channel 2\r\n",
    "paths_for_activate[\"temp_ch2_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2018\\6\"\r\n",
    "paths_for_activate[\"temp_ch2_2018_7\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2018\\7\"\r\n",
    "paths_for_activate[\"temp_ch2_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\2\"\r\n",
    "paths_for_activate[\"temp_ch2_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\3\"\r\n",
    "paths_for_activate[\"temp_ch2_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\4\"\r\n",
    "paths_for_activate[\"temp_ch2_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\5\"\r\n",
    "# channel 3\r\n",
    "paths_for_activate[\"temp_ch3_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2018\\6\"\r\n",
    "paths_for_activate[\"temp_ch3_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\2\"\r\n",
    "paths_for_activate[\"temp_ch3_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\3\"\r\n",
    "paths_for_activate[\"temp_ch3_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\4\"\r\n",
    "paths_for_activate[\"temp_ch3_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\5\"\r\n",
    "# channel 4\r\n",
    "paths_for_activate[\"temp_ch4_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2018\\6\"\r\n",
    "paths_for_activate[\"temp_ch4_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\2\"\r\n",
    "paths_for_activate[\"temp_ch4_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\3\"\r\n",
    "paths_for_activate[\"temp_ch4_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\4\"\r\n",
    "paths_for_activate[\"temp_ch4_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\5\"\r\n",
    "# channel 5\r\n",
    "paths_for_activate[\"temp_ch5_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\5\\Temperature Data\\2018\\6\"\r\n",
    "# channel 6\r\n",
    "paths_for_activate[\"temp_ch6_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\6\\Temperature Data\\2018\\6\"\r\n",
    "# channel 7\r\n",
    "paths_for_activate[\"temp_ch7_2018_11\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\7\\Temperature Data\\2018\\11\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def change_format(one_file,check_date=False):\r\n",
    "    \"\"\"\r\n",
    "    changes the format of the imported data\r\n",
    "\r\n",
    "    Only suited for the specific data used in this notebook\r\n",
    "    \"\"\"\r\n",
    "    one_file = one_file.drop(columns=[one_file.columns[1], one_file.columns[2]])\r\n",
    "    one_file = one_file.rename(columns={one_file.columns[0]: \"Date\"})\r\n",
    "    #Turn the df as is expected to be\r\n",
    "    one_file = pd.melt(one_file, id_vars=\"Date\", var_name=\"Depth [m]\", value_name=\"Temperature\")\r\n",
    "    #errors=\"ignore\" #Creating a time series\r\n",
    "    one_file[\"Date\"] = pd.to_datetime(one_file[\"Date\"], infer_datetime_format=True)\r\n",
    "    #Converting an object to float\r\n",
    "    one_file[\"Depth [m]\"] = one_file[\"Depth [m]\"].astype(float)\r\n",
    "    # date is the same for all\r\n",
    "    one_file = one_file.set_index([\"Depth [m]\"])\r\n",
    "\r\n",
    "    # Very inefficient (computational) check if the file only contatains the same date\r\n",
    "    # because the column will be dropped in the next step\r\n",
    "    if check_date==True: #takes a lot of time\r\n",
    "        for date in one_file[\"Date\"]:\r\n",
    "            for other_date in one_file[\"Date\"]:\r\n",
    "                if date != other_date:\r\n",
    "                    print(\"Error: the file contains data from different dates\")\r\n",
    "\r\n",
    "    # Only one date remains, if check_date=False it is not checked if the dates are the same!\r\n",
    "    # I checked it for a few files an for these all the dates have been the same    \r\n",
    "    one_file.rename(columns = {'Temperature' : one_file[\"Date\"][0]}, inplace = True)\r\n",
    "    one_file=one_file.drop(columns=[\"Date\"])\r\n",
    "\r\n",
    "    return one_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculation - Most important cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%time\r\n",
    "# with all: Wall time: 3h 44s on my i3 laptop\r\n",
    "\r\n",
    "#Input\r\n",
    "more_output=False # True False\r\n",
    "\r\n",
    "#Calculation\r\n",
    "counter=0\r\n",
    "for name in activate:\r\n",
    "    print(\"-----------------------------------------------------------------------\")\r\n",
    "    my_path=paths_for_activate[name]\r\n",
    "    all_my_paths=get_abspath(my_path + \"\\*\\*\") # contains all paths to every file in the activated repository\r\n",
    "    print(\"Start processing %.20s:\"%name)\r\n",
    "    # In some (a lot) files are at the end (multiple) duplicates of the data. \r\n",
    "    # The data is checked if these are realy duplicates. \r\n",
    "\r\n",
    "    all_files_len={} # number of lines in every file in the activated repository\r\n",
    "    for file in all_my_paths:\r\n",
    "        all_files_len[file] = file_len(file)\r\n",
    "\r\n",
    "    #at the moment the notebook only works for 9, 18, 36!!! (or subset of these)\r\n",
    "    for length in set(all_files_len.values()):\r\n",
    "        if length not in {9, 18, 36}:\r\n",
    "            print(\"ERROR: Script needs to be adapted for a new file length\")\r\n",
    "            print(\"length: %.5f\"%length)\r\n",
    "\r\n",
    "    n_files=number_files(my_path)\r\n",
    "    print(\"number of files which will be compiled: %.1f\"%n_files)\r\n",
    "\r\n",
    "    # Read data, change format, compile and create new dataframe\r\n",
    "    print(\"This may take some time\")\r\n",
    "    # start dataframe mustnt be empty, a little bit dirty but it works\r\n",
    "    all_files = pd.read_csv(path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\6\\25\\2018-06-25-22-19-51.txt\",skiprows=7)\r\n",
    "    # Change Format\r\n",
    "    all_files=change_format(all_files)\r\n",
    "    all_files = all_files.rename(columns={all_files.columns[0]: \"dupli\"})\r\n",
    "\r\n",
    "    if more_output==True:\r\n",
    "        counter_process=0\r\n",
    "        print(\"completion in %: not necessary proportional to computation time\")\r\n",
    "    for file in all_my_paths:\r\n",
    "        if more_output==True:\r\n",
    "            if counter_process % 20 == 0: # to get a feeling of remaining computation time\r\n",
    "                print(\"%.1f \" %(counter_process/n_files*100))\r\n",
    "            counter_process+=1\r\n",
    "        \r\n",
    "        if all_files_len[file]==9: # no duplicates at the end\r\n",
    "            # Read Data\r\n",
    "            one_file = pd.read_csv(file,skiprows=7)\r\n",
    "            # Change Format\r\n",
    "            one_file=change_format(one_file)\r\n",
    "\r\n",
    "        if all_files_len[file]==18: # one duplicates at the end\r\n",
    "            # Read data\r\n",
    "            # skipfooter does not work with default engine\r\n",
    "            first = pd.read_csv(file, skiprows=7, skipfooter=9, engine=\"python\")\r\n",
    "            second = pd.read_csv(file, skiprows=16)\r\n",
    "            # change the format\r\n",
    "            first = change_format(first)\r\n",
    "            second = change_format(second)\r\n",
    "            # Check if all are equal\r\n",
    "            # only checks if the shape and elements are the same. Index and columns names can be different\r\n",
    "            # In this case this is sufficient I would say.\r\n",
    "            if first.equals(second):\r\n",
    "                one_file=first\r\n",
    "            else:\r\n",
    "                print(\"Error: 1\")\r\n",
    "\r\n",
    "        if all_files_len[file]==36: # three duplicates at the end\r\n",
    "            # Read data\r\n",
    "            first = pd.read_csv(file, skiprows=7, skipfooter=27, engine=\"python\")\r\n",
    "            second = pd.read_csv(file, skiprows=16,skipfooter=18, engine=\"python\")\r\n",
    "            third = pd.read_csv(file, skiprows=25, skipfooter=9, engine=\"python\")\r\n",
    "            fourth = pd.read_csv(file, skiprows=34)\r\n",
    "            # change the format\r\n",
    "            first = change_format(first)\r\n",
    "            second = change_format(second)\r\n",
    "            third = change_format(third)\r\n",
    "            fourth = change_format(fourth)\r\n",
    "            # Check if all are equal\r\n",
    "            if first.equals(second) and first.equals(third) and first.equals(fourth):\r\n",
    "                one_file=first\r\n",
    "            else:\r\n",
    "                print(\"Error: 2\")\r\n",
    "\r\n",
    "        all_files=pd.concat([all_files,one_file],axis=1)\r\n",
    "\r\n",
    "    all_files=all_files.drop(columns=\"dupli\") # a bit dirty but it works\r\n",
    "\r\n",
    "    # Save compiled data\r\n",
    "    filename=name + \"_\" + version + \".csv\"\r\n",
    "    counter+=1\r\n",
    "    all_files.to_csv(path_wdir + \"\\\\\" + filename)  \r\n",
    "    print(\"created %.40s\"%filename)\r\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2018_6:\n",
      "number of files which will be compiled: 691.0\n",
      "This may take some time\n",
      "created temp_ch1_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2018_7:\n",
      "number of files which will be compiled: 1944.0\n",
      "This may take some time\n",
      "created temp_ch1_2018_7_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2018_11:\n",
      "number of files which will be compiled: 1238.0\n",
      "This may take some time\n",
      "created temp_ch1_2018_11_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2019_2:\n",
      "number of files which will be compiled: 1486.0\n",
      "This may take some time\n",
      "created temp_ch1_2019_2_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2019_3:\n",
      "number of files which will be compiled: 2788.0\n",
      "This may take some time\n",
      "created temp_ch1_2019_3_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2019_4:\n",
      "number of files which will be compiled: 2698.0\n",
      "This may take some time\n",
      "created temp_ch1_2019_4_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch1_2019_5:\n",
      "number of files which will be compiled: 659.0\n",
      "This may take some time\n",
      "created temp_ch1_2019_5_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch2_2018_6:\n",
      "number of files which will be compiled: 689.0\n",
      "This may take some time\n",
      "created temp_ch2_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch2_2018_7:\n",
      "number of files which will be compiled: 1942.0\n",
      "This may take some time\n",
      "created temp_ch2_2018_7_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch2_2019_2:\n",
      "number of files which will be compiled: 1484.0\n",
      "This may take some time\n",
      "created temp_ch2_2019_2_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch2_2019_3:\n",
      "number of files which will be compiled: 2788.0\n",
      "This may take some time\n",
      "created temp_ch2_2019_3_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch2_2019_4:\n",
      "number of files which will be compiled: 2698.0\n",
      "This may take some time\n",
      "created temp_ch2_2019_4_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch2_2019_5:\n",
      "number of files which will be compiled: 660.0\n",
      "This may take some time\n",
      "created temp_ch2_2019_5_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch3_2018_6:\n",
      "number of files which will be compiled: 29.0\n",
      "This may take some time\n",
      "created temp_ch3_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch3_2019_2:\n",
      "number of files which will be compiled: 1484.0\n",
      "This may take some time\n",
      "created temp_ch3_2019_2_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch3_2019_3:\n",
      "number of files which will be compiled: 2788.0\n",
      "This may take some time\n",
      "created temp_ch3_2019_3_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch3_2019_4:\n",
      "number of files which will be compiled: 2698.0\n",
      "This may take some time\n",
      "created temp_ch3_2019_4_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch3_2019_5:\n",
      "number of files which will be compiled: 660.0\n",
      "This may take some time\n",
      "created temp_ch3_2019_5_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch4_2018_6:\n",
      "number of files which will be compiled: 29.0\n",
      "This may take some time\n",
      "created temp_ch4_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch4_2019_2:\n",
      "number of files which will be compiled: 1483.0\n",
      "This may take some time\n",
      "created temp_ch4_2019_2_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch4_2019_3:\n",
      "number of files which will be compiled: 2788.0\n",
      "This may take some time\n",
      "created temp_ch4_2019_3_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch4_2019_4:\n",
      "number of files which will be compiled: 2698.0\n",
      "This may take some time\n",
      "created temp_ch4_2019_4_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch4_2019_5:\n",
      "number of files which will be compiled: 660.0\n",
      "This may take some time\n",
      "created temp_ch4_2019_5_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch5_2018_6:\n",
      "number of files which will be compiled: 28.0\n",
      "This may take some time\n",
      "created temp_ch5_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch6_2018_6:\n",
      "number of files which will be compiled: 28.0\n",
      "This may take some time\n",
      "created temp_ch6_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch7_2018_11:\n",
      "number of files which will be compiled: 1237.0\n",
      "This may take some time\n",
      "created temp_ch7_2018_11_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "Wall time: 3h 44s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load created data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# load new dataset\r\n",
    "#df=pd.read_csv(filename,index_col=0)\r\n",
    "#df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}