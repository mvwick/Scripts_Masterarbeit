{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd029bb46dac4ac1939543a1997a987c8ba0e6eacd9d5e001c58572fbace647ecc5",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Merge Files\n",
    "This script works with data in `Masterarbeit\\Alsdorf\\Daten\\Daten_Mario_oder_alt`\n",
    "\n",
    "This script converts the data exported with charon4, into a better format for processing them with python.\n",
    "This script merges multiple smaller files into a bigger one.\n",
    "\n",
    "the final filename: `temp_chx_year_month_vx.csv`\n",
    "* temp: Temperature\n",
    "* chx: Channel, x: number\n",
    "* year\n",
    "* month\n",
    "* vx: version of this script\n",
    "\n",
    "example files contains an example file which can be processed with this script. The main folder, which contains all files (about 500000) is `.zip`\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "version=\"v1_2\" # will be used for naming the final file.\n",
    "# Changelog\n",
    "#1_0: initial version\n",
    "#1_1: removed all as input, now a list is used\n",
    "#1_2: changed the position of the script, added path_wdir variable; made a test run only for one activated input\n",
    "#\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die die weniger messungen enthalten könnten evntl gekürzt worden sein, dann wäre die Tiefe falsch. Z.b. jeder 10te Wert aus dem uhrsprünglichen Datensatz\n",
    "\n",
    "# überprüfung datum deaktiviert, da es sehr lange dauert!!!!!!!!!!!!!!\n",
    "\n",
    "# sollte evntl auch als pickle speichern ist glaube ich effizienter oder h5py?\n",
    "\n",
    "# wenn hier alles fertig ist grafik erstellen die zeigt wo Daten verfügbar sind\n",
    "\n",
    "# daten sind teilweise neagtiv? müssen noch sortiert werden / auf plausibilität geprüft werden\n",
    "\n",
    "# Daten angucken die ich bisher nicht hier drin habe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_inputs=['temp_ch1_2018_6', 'temp_ch1_2018_7', 'temp_ch1_2018_11', 'temp_ch1_2019_2', 'temp_ch1_2019_3', 'temp_ch1_2019_4', 'temp_ch1_2019_5', 'temp_ch2_2018_6', 'temp_ch2_2018_7', 'temp_ch2_2019_2', 'temp_ch2_2019_3', 'temp_ch2_2019_4', 'temp_ch2_2019_5', 'temp_ch3_2018_6', 'temp_ch3_2019_2', 'temp_ch3_2019_3', 'temp_ch3_2019_4', 'temp_ch3_2019_5', 'temp_ch4_2018_6', 'temp_ch4_2019_2', 'temp_ch4_2019_3', 'temp_ch4_2019_4', 'temp_ch4_2019_5', 'temp_ch5_2018_6', 'temp_ch6_2018_6', 'temp_ch7_2018_11']\n",
    "\n",
    "############################## Input############################################\n",
    "# one or more of activate_inputs\n",
    "# using all takes very long and may not use the most up-to-date version of this notebook\n",
    "activate=activate_inputs[-3]\n",
    "#Example: activate_inputs[:]; activate_inputs[5]; activate_inputs[22:25]\n",
    "path_wdir=r\"..\\Alsdorf\\Daten\\Daten_Mario_oder_alt\" #path to working directory\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(activate) == str: # it is later assumed activate is a list of names\n",
    "    activate=[activate]\n",
    "\n",
    "# path which will be activated\n",
    "# all files in each of these paths will be merged\n",
    "paths_for_activate={}\n",
    "# channel 1\n",
    "paths_for_activate[\"temp_ch1_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\6\"\n",
    "paths_for_activate[\"temp_ch1_2018_7\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\7\"\n",
    "paths_for_activate[\"temp_ch1_2018_11\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\11\"\n",
    "paths_for_activate[\"temp_ch1_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\2\"\n",
    "paths_for_activate[\"temp_ch1_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\3\"\n",
    "paths_for_activate[\"temp_ch1_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\4\"\n",
    "paths_for_activate[\"temp_ch1_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2019\\5\"\n",
    "# channel 2\n",
    "paths_for_activate[\"temp_ch2_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2018\\6\"\n",
    "paths_for_activate[\"temp_ch2_2018_7\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2018\\7\"\n",
    "paths_for_activate[\"temp_ch2_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\2\"\n",
    "paths_for_activate[\"temp_ch2_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\3\"\n",
    "paths_for_activate[\"temp_ch2_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\4\"\n",
    "paths_for_activate[\"temp_ch2_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\2\\Temperature Data\\2019\\5\"\n",
    "# channel 3\n",
    "paths_for_activate[\"temp_ch3_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2018\\6\"\n",
    "paths_for_activate[\"temp_ch3_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\2\"\n",
    "paths_for_activate[\"temp_ch3_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\3\"\n",
    "paths_for_activate[\"temp_ch3_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\4\"\n",
    "paths_for_activate[\"temp_ch3_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\3\\Temperature Data\\2019\\5\"\n",
    "# channel 4\n",
    "paths_for_activate[\"temp_ch4_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2018\\6\"\n",
    "paths_for_activate[\"temp_ch4_2019_2\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\2\"\n",
    "paths_for_activate[\"temp_ch4_2019_3\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\3\"\n",
    "paths_for_activate[\"temp_ch4_2019_4\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\4\"\n",
    "paths_for_activate[\"temp_ch4_2019_5\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\4\\Temperature Data\\2019\\5\"\n",
    "# channel 5\n",
    "paths_for_activate[\"temp_ch5_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\5\\Temperature Data\\2018\\6\"\n",
    "# channel 6\n",
    "paths_for_activate[\"temp_ch6_2018_6\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\6\\Temperature Data\\2018\\6\"\n",
    "# channel 7\n",
    "paths_for_activate[\"temp_ch7_2018_11\"]=path_wdir + r\"\\desktop-folder\\Controller\\3188\\7\\Temperature Data\\2018\\11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_format(one_file,check_date=False):\n",
    "    \"\"\"\n",
    "    changes the format of the imported data\n",
    "\n",
    "    Only suited for the specific data used in this notebook\n",
    "    \"\"\"\n",
    "    one_file = one_file.drop(columns=[one_file.columns[1], one_file.columns[2]])\n",
    "    one_file = one_file.rename(columns={one_file.columns[0]: \"Date\"})\n",
    "    #Turn the df as is expected to be\n",
    "    one_file = pd.melt(one_file, id_vars=\"Date\", var_name=\"Depth [m]\", value_name=\"Temperature\")\n",
    "    #errors=\"ignore\" #Creating a time series\n",
    "    one_file[\"Date\"] = pd.to_datetime(one_file[\"Date\"], infer_datetime_format=True)\n",
    "    #Converting an object to float\n",
    "    one_file[\"Depth [m]\"] = one_file[\"Depth [m]\"].astype(float)\n",
    "    # date is the same for all\n",
    "    one_file = one_file.set_index([\"Depth [m]\"])\n",
    "\n",
    "    # Very inefficient (computational) check if the file only contatains the same date\n",
    "    # because the column will be dropped in the next step\n",
    "    if check_date==True: #takes a lot of time\n",
    "        for date in one_file[\"Date\"]:\n",
    "            for other_date in one_file[\"Date\"]:\n",
    "                if date != other_date:\n",
    "                    print(\"Error: the file contains data from different dates\")\n",
    "\n",
    "    # Only one date remains, if check_date=False it is not checked if the dates are the same!\n",
    "    # I checked it for a few files an for these all the dates have been the same    \n",
    "    one_file.rename(columns = {'Temperature' : one_file[\"Date\"][0]}, inplace = True)\n",
    "    one_file=one_file.drop(columns=[\"Date\"])\n",
    "\n",
    "    return one_file\n",
    "\n",
    "def get_abspath(basepath):\n",
    "    \"\"\"Get the files you need to import into your script with Path\n",
    "    Returns a list of all filepaths (or folderpaths) of the files (or folders) in a repository.\n",
    "    The repository is given this function with basepath.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    basepath = glob.glob(basepath)\n",
    "    for entry in basepath:\n",
    "        df_list.append(entry)\n",
    "    return (df_list)\n",
    "\n",
    "def file_len(fname):\n",
    "    \"\"\"returns the number of lines of a file\"\"\"\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "def number_files(path):\n",
    "    \"\"\"counts the number of files in the repository\"\"\"\n",
    "    total = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        total += len(files)\n",
    "    return total\n"
   ]
  },
  {
   "source": [
    "# Calculation - Most important cell\n",
    "here the actual calculation is done. Maybe I should create a User-Defined-Function for this. But at the moment this is suited I think."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Start processing temp_ch5_2018_6:\n",
      "number of files which will be compiled: 28.0\n",
      "This may take some time\n",
      "created temp_ch5_2018_6_v1_2.csv\n",
      "-----------------------------------------------------------------------\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with all: Wall time: 2h 4min 9s\n",
    "\n",
    "#Input\n",
    "more_output=False # True False\n",
    "\n",
    "#Calculation\n",
    "counter=0\n",
    "for name in activate:\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    my_path=paths_for_activate[name]\n",
    "    all_my_paths=get_abspath(my_path + \"\\*\\*\") # contains all paths to every file in the activated repository\n",
    "    print(\"Start processing %.20s:\"%name)\n",
    "    # In some (a lot) files are at the end (multiple) duplicates of the data. \n",
    "    # The data is checked if these are realy duplicates. \n",
    "\n",
    "    all_files_len={} # number of lines in every file in the activated repository\n",
    "    for file in all_my_paths:\n",
    "        all_files_len[file] = file_len(file)\n",
    "\n",
    "    #at the moment the notebook only works for 9, 18, 36!!! (or subset of these)\n",
    "    for length in set(all_files_len.values()):\n",
    "        if length not in {9, 18, 36}:\n",
    "            print(\"ERROR: Script needs to be adapted for a new file length\")\n",
    "            print(\"length: %.5f\"%length)\n",
    "\n",
    "    n_files=number_files(my_path)\n",
    "    print(\"number of files which will be compiled: %.1f\"%n_files)\n",
    "\n",
    "    # Read data, change format, compile and create new dataframe\n",
    "    print(\"This may take some time\")\n",
    "    # start dataframe mustnt be empty, a little bit dirty but it works\n",
    "    all_files = pd.read_csv(path_wdir + r\"\\desktop-folder\\Controller\\3188\\1\\Temperature Data\\2018\\6\\25\\2018-06-25-22-19-51.txt\",skiprows=7)\n",
    "    # Change Format\n",
    "    all_files=change_format(all_files)\n",
    "    all_files = all_files.rename(columns={all_files.columns[0]: \"dupli\"})\n",
    "\n",
    "    if more_output==True:\n",
    "        counter_process=0\n",
    "        print(\"completion in %: not necessary proportional to computation time\")\n",
    "    for file in all_my_paths:\n",
    "        if more_output==True:\n",
    "            if counter_process % 20 == 0: # to get a feeling of remaining computation time\n",
    "                print(\"%.1f \" %(counter_process/n_files*100))\n",
    "            counter_process+=1\n",
    "        \n",
    "        if all_files_len[file]==9: # no duplicates at the end\n",
    "            # Read Data\n",
    "            one_file = pd.read_csv(file,skiprows=7)\n",
    "            # Change Format\n",
    "            one_file=change_format(one_file)\n",
    "\n",
    "        if all_files_len[file]==18: # one duplicates at the end\n",
    "            # Read data\n",
    "            # skipfooter does not work with default engine\n",
    "            first = pd.read_csv(file, skiprows=7, skipfooter=9, engine=\"python\")\n",
    "            second = pd.read_csv(file, skiprows=16)\n",
    "            # change the format\n",
    "            first = change_format(first)\n",
    "            second = change_format(second)\n",
    "            # Check if all are equal\n",
    "            # only checks if the shape and elements are the same. Index and columns names can be different\n",
    "            # In this case this is sufficient I would say.\n",
    "            if first.equals(second):\n",
    "                one_file=first\n",
    "            else:\n",
    "                print(\"Error: 1\")\n",
    "\n",
    "        if all_files_len[file]==36: # three duplicates at the end\n",
    "            # Read data\n",
    "            first = pd.read_csv(file, skiprows=7, skipfooter=27, engine=\"python\")\n",
    "            second = pd.read_csv(file, skiprows=16,skipfooter=18, engine=\"python\")\n",
    "            third = pd.read_csv(file, skiprows=25, skipfooter=9, engine=\"python\")\n",
    "            fourth = pd.read_csv(file, skiprows=34)\n",
    "            # change the format\n",
    "            first = change_format(first)\n",
    "            second = change_format(second)\n",
    "            third = change_format(third)\n",
    "            fourth = change_format(fourth)\n",
    "            # Check if all are equal\n",
    "            if first.equals(second) and first.equals(third) and first.equals(fourth):\n",
    "                one_file=first\n",
    "            else:\n",
    "                print(\"Error: 2\")\n",
    "\n",
    "        all_files=pd.concat([all_files,one_file],axis=1)\n",
    "\n",
    "    all_files=all_files.drop(columns=\"dupli\") # a bit dirty but it works\n",
    "\n",
    "    # Save compiled data\n",
    "    filename=name + \"_\" + version + \".csv\"\n",
    "    counter+=1\n",
    "    all_files.to_csv(path_wdir + \"\\\\\" + filename)  \n",
    "    print(\"created %.40s\"%filename)\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "source": [
    "# Load created data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new dataset\n",
    "#df=pd.read_csv(filename,index_col=0)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}